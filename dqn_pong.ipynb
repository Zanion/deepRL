{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers\n",
    "\n",
    "Obtained from https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "\n",
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conf_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        \n",
    "    def _get_conf_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
    "MEAN_REWARD_BOUND = 19.5\n",
    "\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 1000\n",
    "REPLAY_START_SIZE = 10000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 10**5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "\n",
    "\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.state = env.reset()\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a).to(device)\n",
    "            q_vals_v = net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "\n",
    "\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ProcessFrame84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    return ScaledFloatFrame(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "851: done 1 games, mean reward -21.000, eps 0.99, speed 642.39 f/s\n",
      "1806: done 2 games, mean reward -20.000, eps 0.98, speed 617.86 f/s\n",
      "Best mean reward updated -21.000 -> -20.000, model saved\n",
      "2744: done 3 games, mean reward -20.333, eps 0.97, speed 623.05 f/s\n",
      "3654: done 4 games, mean reward -20.500, eps 0.96, speed 628.08 f/s\n",
      "4534: done 5 games, mean reward -20.600, eps 0.95, speed 624.29 f/s\n",
      "5352: done 6 games, mean reward -20.667, eps 0.95, speed 623.61 f/s\n",
      "6114: done 7 games, mean reward -20.714, eps 0.94, speed 612.55 f/s\n",
      "6982: done 8 games, mean reward -20.625, eps 0.93, speed 616.35 f/s\n",
      "7744: done 9 games, mean reward -20.667, eps 0.92, speed 611.52 f/s\n",
      "8506: done 10 games, mean reward -20.700, eps 0.91, speed 615.37 f/s\n",
      "9316: done 11 games, mean reward -20.727, eps 0.91, speed 599.94 f/s\n",
      "10078: done 12 games, mean reward -20.750, eps 0.90, speed 366.63 f/s\n",
      "10947: done 13 games, mean reward -20.692, eps 0.89, speed 86.04 f/s\n",
      "11816: done 14 games, mean reward -20.643, eps 0.88, speed 85.21 f/s\n",
      "12827: done 15 games, mean reward -20.600, eps 0.87, speed 84.69 f/s\n",
      "13670: done 16 games, mean reward -20.625, eps 0.86, speed 84.48 f/s\n",
      "14432: done 17 games, mean reward -20.647, eps 0.86, speed 84.92 f/s\n",
      "15254: done 18 games, mean reward -20.667, eps 0.85, speed 85.94 f/s\n",
      "16117: done 19 games, mean reward -20.632, eps 0.84, speed 85.38 f/s\n",
      "17025: done 20 games, mean reward -20.650, eps 0.83, speed 84.99 f/s\n",
      "17955: done 21 games, mean reward -20.667, eps 0.82, speed 86.29 f/s\n",
      "18841: done 22 games, mean reward -20.636, eps 0.81, speed 85.24 f/s\n",
      "19991: done 23 games, mean reward -20.522, eps 0.80, speed 84.75 f/s\n",
      "20832: done 24 games, mean reward -20.500, eps 0.79, speed 84.79 f/s\n",
      "21594: done 25 games, mean reward -20.520, eps 0.78, speed 85.19 f/s\n",
      "22545: done 26 games, mean reward -20.500, eps 0.77, speed 85.79 f/s\n",
      "23475: done 27 games, mean reward -20.481, eps 0.77, speed 85.24 f/s\n",
      "24453: done 28 games, mean reward -20.429, eps 0.76, speed 85.67 f/s\n",
      "25426: done 29 games, mean reward -20.448, eps 0.75, speed 84.27 f/s\n",
      "26436: done 30 games, mean reward -20.433, eps 0.74, speed 85.24 f/s\n",
      "27417: done 31 games, mean reward -20.419, eps 0.73, speed 83.38 f/s\n",
      "28267: done 32 games, mean reward -20.438, eps 0.72, speed 84.73 f/s\n",
      "29394: done 33 games, mean reward -20.424, eps 0.71, speed 84.86 f/s\n",
      "30629: done 34 games, mean reward -20.294, eps 0.69, speed 85.24 f/s\n",
      "31985: done 35 games, mean reward -20.229, eps 0.68, speed 85.29 f/s\n",
      "33040: done 36 games, mean reward -20.222, eps 0.67, speed 83.65 f/s\n",
      "34266: done 37 games, mean reward -20.135, eps 0.66, speed 84.54 f/s\n",
      "35381: done 38 games, mean reward -20.105, eps 0.65, speed 84.63 f/s\n",
      "36683: done 39 games, mean reward -20.026, eps 0.63, speed 84.36 f/s\n",
      "37710: done 40 games, mean reward -20.025, eps 0.62, speed 85.05 f/s\n",
      "38640: done 41 games, mean reward -20.049, eps 0.61, speed 83.09 f/s\n",
      "39509: done 42 games, mean reward -20.071, eps 0.60, speed 83.86 f/s\n",
      "40735: done 43 games, mean reward -20.093, eps 0.59, speed 83.32 f/s\n",
      "41723: done 44 games, mean reward -20.114, eps 0.58, speed 82.51 f/s\n",
      "42951: done 45 games, mean reward -20.089, eps 0.57, speed 83.48 f/s\n",
      "44018: done 46 games, mean reward -20.043, eps 0.56, speed 82.65 f/s\n",
      "45053: done 47 games, mean reward -20.043, eps 0.55, speed 82.23 f/s\n",
      "46117: done 48 games, mean reward -20.042, eps 0.54, speed 81.51 f/s\n",
      "47592: done 49 games, mean reward -20.000, eps 0.52, speed 82.36 f/s\n",
      "48951: done 50 games, mean reward -19.940, eps 0.51, speed 81.76 f/s\n",
      "Best mean reward updated -20.000 -> -19.940, model saved\n",
      "50200: done 51 games, mean reward -19.922, eps 0.50, speed 81.76 f/s\n",
      "Best mean reward updated -19.940 -> -19.922, model saved\n",
      "51631: done 52 games, mean reward -19.885, eps 0.48, speed 83.15 f/s\n",
      "Best mean reward updated -19.922 -> -19.885, model saved\n",
      "53045: done 53 games, mean reward -19.887, eps 0.47, speed 82.81 f/s\n",
      "54305: done 54 games, mean reward -19.852, eps 0.46, speed 82.48 f/s\n",
      "Best mean reward updated -19.885 -> -19.852, model saved\n",
      "55394: done 55 games, mean reward -19.836, eps 0.45, speed 80.28 f/s\n",
      "Best mean reward updated -19.852 -> -19.836, model saved\n",
      "56769: done 56 games, mean reward -19.821, eps 0.43, speed 79.69 f/s\n",
      "Best mean reward updated -19.836 -> -19.821, model saved\n",
      "57805: done 57 games, mean reward -19.807, eps 0.42, speed 83.30 f/s\n",
      "Best mean reward updated -19.821 -> -19.807, model saved\n",
      "59186: done 58 games, mean reward -19.776, eps 0.41, speed 83.45 f/s\n",
      "Best mean reward updated -19.807 -> -19.776, model saved\n",
      "60229: done 59 games, mean reward -19.797, eps 0.40, speed 81.94 f/s\n",
      "61579: done 60 games, mean reward -19.783, eps 0.38, speed 82.28 f/s\n",
      "62805: done 61 games, mean reward -19.770, eps 0.37, speed 82.90 f/s\n",
      "Best mean reward updated -19.776 -> -19.770, model saved\n",
      "64330: done 62 games, mean reward -19.742, eps 0.36, speed 83.28 f/s\n",
      "Best mean reward updated -19.770 -> -19.742, model saved\n",
      "65509: done 63 games, mean reward -19.746, eps 0.34, speed 82.91 f/s\n",
      "66921: done 64 games, mean reward -19.734, eps 0.33, speed 84.21 f/s\n",
      "Best mean reward updated -19.742 -> -19.734, model saved\n",
      "68953: done 65 games, mean reward -19.692, eps 0.31, speed 82.40 f/s\n",
      "Best mean reward updated -19.734 -> -19.692, model saved\n",
      "70525: done 66 games, mean reward -19.621, eps 0.29, speed 81.88 f/s\n",
      "Best mean reward updated -19.692 -> -19.621, model saved\n",
      "71845: done 67 games, mean reward -19.612, eps 0.28, speed 82.68 f/s\n",
      "Best mean reward updated -19.621 -> -19.612, model saved\n",
      "73687: done 68 games, mean reward -19.559, eps 0.26, speed 82.50 f/s\n",
      "Best mean reward updated -19.612 -> -19.559, model saved\n",
      "75449: done 69 games, mean reward -19.507, eps 0.25, speed 82.53 f/s\n",
      "Best mean reward updated -19.559 -> -19.507, model saved\n",
      "77385: done 70 games, mean reward -19.443, eps 0.23, speed 81.66 f/s\n",
      "Best mean reward updated -19.507 -> -19.443, model saved\n",
      "78767: done 71 games, mean reward -19.451, eps 0.21, speed 82.12 f/s\n",
      "80451: done 72 games, mean reward -19.403, eps 0.20, speed 81.85 f/s\n",
      "Best mean reward updated -19.443 -> -19.403, model saved\n",
      "82206: done 73 games, mean reward -19.370, eps 0.18, speed 81.51 f/s\n",
      "Best mean reward updated -19.403 -> -19.370, model saved\n",
      "84273: done 74 games, mean reward -19.243, eps 0.16, speed 81.01 f/s\n",
      "Best mean reward updated -19.370 -> -19.243, model saved\n",
      "86111: done 75 games, mean reward -19.200, eps 0.14, speed 81.78 f/s\n",
      "Best mean reward updated -19.243 -> -19.200, model saved\n",
      "87343: done 76 games, mean reward -19.224, eps 0.13, speed 80.79 f/s\n",
      "89283: done 77 games, mean reward -19.130, eps 0.11, speed 80.85 f/s\n",
      "Best mean reward updated -19.200 -> -19.130, model saved\n",
      "91385: done 78 games, mean reward -19.115, eps 0.09, speed 81.65 f/s\n",
      "Best mean reward updated -19.130 -> -19.115, model saved\n",
      "93214: done 79 games, mean reward -19.076, eps 0.07, speed 81.49 f/s\n",
      "Best mean reward updated -19.115 -> -19.076, model saved\n",
      "94945: done 80 games, mean reward -19.050, eps 0.05, speed 81.26 f/s\n",
      "Best mean reward updated -19.076 -> -19.050, model saved\n",
      "97046: done 81 games, mean reward -18.988, eps 0.03, speed 80.64 f/s\n",
      "Best mean reward updated -19.050 -> -18.988, model saved\n",
      "98563: done 82 games, mean reward -18.988, eps 0.02, speed 79.75 f/s\n",
      "100344: done 83 games, mean reward -19.000, eps 0.02, speed 80.69 f/s\n",
      "102040: done 84 games, mean reward -18.988, eps 0.02, speed 80.11 f/s\n",
      "103670: done 85 games, mean reward -18.976, eps 0.02, speed 80.60 f/s\n",
      "Best mean reward updated -18.988 -> -18.976, model saved\n",
      "105711: done 86 games, mean reward -18.953, eps 0.02, speed 80.19 f/s\n",
      "Best mean reward updated -18.976 -> -18.953, model saved\n",
      "108269: done 87 games, mean reward -18.885, eps 0.02, speed 80.86 f/s\n",
      "Best mean reward updated -18.953 -> -18.885, model saved\n",
      "110649: done 88 games, mean reward -18.841, eps 0.02, speed 81.22 f/s\n",
      "Best mean reward updated -18.885 -> -18.841, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112887: done 89 games, mean reward -18.798, eps 0.02, speed 80.30 f/s\n",
      "Best mean reward updated -18.841 -> -18.798, model saved\n",
      "115682: done 90 games, mean reward -18.678, eps 0.02, speed 80.76 f/s\n",
      "Best mean reward updated -18.798 -> -18.678, model saved\n",
      "118978: done 91 games, mean reward -18.527, eps 0.02, speed 80.86 f/s\n",
      "Best mean reward updated -18.678 -> -18.527, model saved\n",
      "121778: done 92 games, mean reward -18.413, eps 0.02, speed 80.95 f/s\n",
      "Best mean reward updated -18.527 -> -18.413, model saved\n",
      "124455: done 93 games, mean reward -18.323, eps 0.02, speed 80.76 f/s\n",
      "Best mean reward updated -18.413 -> -18.323, model saved\n",
      "127152: done 94 games, mean reward -18.255, eps 0.02, speed 80.50 f/s\n",
      "Best mean reward updated -18.323 -> -18.255, model saved\n",
      "130017: done 95 games, mean reward -18.158, eps 0.02, speed 80.74 f/s\n",
      "Best mean reward updated -18.255 -> -18.158, model saved\n",
      "133040: done 96 games, mean reward -18.042, eps 0.02, speed 80.81 f/s\n",
      "Best mean reward updated -18.158 -> -18.042, model saved\n",
      "135544: done 97 games, mean reward -17.948, eps 0.02, speed 80.79 f/s\n",
      "Best mean reward updated -18.042 -> -17.948, model saved\n",
      "138211: done 98 games, mean reward -17.898, eps 0.02, speed 80.91 f/s\n",
      "Best mean reward updated -17.948 -> -17.898, model saved\n",
      "141318: done 99 games, mean reward -17.818, eps 0.02, speed 81.08 f/s\n",
      "Best mean reward updated -17.898 -> -17.818, model saved\n",
      "144533: done 100 games, mean reward -17.720, eps 0.02, speed 80.66 f/s\n",
      "Best mean reward updated -17.818 -> -17.720, model saved\n",
      "147245: done 101 games, mean reward -17.640, eps 0.02, speed 80.64 f/s\n",
      "Best mean reward updated -17.720 -> -17.640, model saved\n",
      "149843: done 102 games, mean reward -17.600, eps 0.02, speed 80.62 f/s\n",
      "Best mean reward updated -17.640 -> -17.600, model saved\n",
      "152844: done 103 games, mean reward -17.500, eps 0.02, speed 80.65 f/s\n",
      "Best mean reward updated -17.600 -> -17.500, model saved\n",
      "155835: done 104 games, mean reward -17.360, eps 0.02, speed 80.43 f/s\n",
      "Best mean reward updated -17.500 -> -17.360, model saved\n",
      "159046: done 105 games, mean reward -17.250, eps 0.02, speed 80.47 f/s\n",
      "Best mean reward updated -17.360 -> -17.250, model saved\n",
      "162454: done 106 games, mean reward -17.140, eps 0.02, speed 80.81 f/s\n",
      "Best mean reward updated -17.250 -> -17.140, model saved\n",
      "166943: done 107 games, mean reward -16.970, eps 0.02, speed 80.73 f/s\n",
      "Best mean reward updated -17.140 -> -16.970, model saved\n",
      "171593: done 108 games, mean reward -16.810, eps 0.02, speed 80.68 f/s\n",
      "Best mean reward updated -16.970 -> -16.810, model saved\n",
      "175337: done 109 games, mean reward -16.620, eps 0.02, speed 80.90 f/s\n",
      "Best mean reward updated -16.810 -> -16.620, model saved\n",
      "179216: done 110 games, mean reward -16.390, eps 0.02, speed 80.74 f/s\n",
      "Best mean reward updated -16.620 -> -16.390, model saved\n",
      "183390: done 111 games, mean reward -16.150, eps 0.02, speed 80.35 f/s\n",
      "Best mean reward updated -16.390 -> -16.150, model saved\n",
      "187726: done 112 games, mean reward -15.960, eps 0.02, speed 80.49 f/s\n",
      "Best mean reward updated -16.150 -> -15.960, model saved\n",
      "191494: done 113 games, mean reward -15.750, eps 0.02, speed 80.66 f/s\n",
      "Best mean reward updated -15.960 -> -15.750, model saved\n",
      "195182: done 114 games, mean reward -15.520, eps 0.02, speed 80.84 f/s\n",
      "Best mean reward updated -15.750 -> -15.520, model saved\n",
      "198514: done 115 games, mean reward -15.220, eps 0.02, speed 80.95 f/s\n",
      "Best mean reward updated -15.520 -> -15.220, model saved\n",
      "201824: done 116 games, mean reward -14.940, eps 0.02, speed 80.94 f/s\n",
      "Best mean reward updated -15.220 -> -14.940, model saved\n",
      "204974: done 117 games, mean reward -14.650, eps 0.02, speed 81.39 f/s\n",
      "Best mean reward updated -14.940 -> -14.650, model saved\n",
      "208753: done 118 games, mean reward -14.430, eps 0.02, speed 80.67 f/s\n",
      "Best mean reward updated -14.650 -> -14.430, model saved\n",
      "212602: done 119 games, mean reward -14.260, eps 0.02, speed 80.79 f/s\n",
      "Best mean reward updated -14.430 -> -14.260, model saved\n",
      "216067: done 120 games, mean reward -14.000, eps 0.02, speed 80.67 f/s\n",
      "Best mean reward updated -14.260 -> -14.000, model saved\n",
      "219331: done 121 games, mean reward -13.720, eps 0.02, speed 81.03 f/s\n",
      "Best mean reward updated -14.000 -> -13.720, model saved\n",
      "222776: done 122 games, mean reward -13.440, eps 0.02, speed 80.45 f/s\n",
      "Best mean reward updated -13.720 -> -13.440, model saved\n",
      "225562: done 123 games, mean reward -13.140, eps 0.02, speed 81.08 f/s\n",
      "Best mean reward updated -13.440 -> -13.140, model saved\n",
      "227719: done 124 games, mean reward -12.730, eps 0.02, speed 80.56 f/s\n",
      "Best mean reward updated -13.140 -> -12.730, model saved\n",
      "231151: done 125 games, mean reward -12.540, eps 0.02, speed 80.66 f/s\n",
      "Best mean reward updated -12.730 -> -12.540, model saved\n",
      "233293: done 126 games, mean reward -12.160, eps 0.02, speed 79.96 f/s\n",
      "Best mean reward updated -12.540 -> -12.160, model saved\n",
      "236763: done 127 games, mean reward -11.920, eps 0.02, speed 80.40 f/s\n",
      "Best mean reward updated -12.160 -> -11.920, model saved\n",
      "240117: done 128 games, mean reward -11.720, eps 0.02, speed 80.91 f/s\n",
      "Best mean reward updated -11.920 -> -11.720, model saved\n",
      "243240: done 129 games, mean reward -11.410, eps 0.02, speed 80.67 f/s\n",
      "Best mean reward updated -11.720 -> -11.410, model saved\n",
      "246409: done 130 games, mean reward -11.150, eps 0.02, speed 80.95 f/s\n",
      "Best mean reward updated -11.410 -> -11.150, model saved\n",
      "249458: done 131 games, mean reward -10.830, eps 0.02, speed 80.34 f/s\n",
      "Best mean reward updated -11.150 -> -10.830, model saved\n",
      "252564: done 132 games, mean reward -10.560, eps 0.02, speed 80.82 f/s\n",
      "Best mean reward updated -10.830 -> -10.560, model saved\n",
      "255770: done 133 games, mean reward -10.280, eps 0.02, speed 81.04 f/s\n",
      "Best mean reward updated -10.560 -> -10.280, model saved\n",
      "258266: done 134 games, mean reward -9.990, eps 0.02, speed 80.56 f/s\n",
      "Best mean reward updated -10.280 -> -9.990, model saved\n",
      "261320: done 135 games, mean reward -9.730, eps 0.02, speed 80.62 f/s\n",
      "Best mean reward updated -9.990 -> -9.730, model saved\n",
      "263711: done 136 games, mean reward -9.400, eps 0.02, speed 80.52 f/s\n",
      "Best mean reward updated -9.730 -> -9.400, model saved\n",
      "266577: done 137 games, mean reward -9.120, eps 0.02, speed 80.72 f/s\n",
      "Best mean reward updated -9.400 -> -9.120, model saved\n",
      "269414: done 138 games, mean reward -8.810, eps 0.02, speed 81.16 f/s\n",
      "Best mean reward updated -9.120 -> -8.810, model saved\n",
      "271827: done 139 games, mean reward -8.480, eps 0.02, speed 80.60 f/s\n",
      "Best mean reward updated -8.810 -> -8.480, model saved\n",
      "274809: done 140 games, mean reward -8.160, eps 0.02, speed 81.06 f/s\n",
      "Best mean reward updated -8.480 -> -8.160, model saved\n",
      "277485: done 141 games, mean reward -7.820, eps 0.02, speed 80.49 f/s\n",
      "Best mean reward updated -8.160 -> -7.820, model saved\n",
      "280566: done 142 games, mean reward -7.500, eps 0.02, speed 79.97 f/s\n",
      "Best mean reward updated -7.820 -> -7.500, model saved\n",
      "282885: done 143 games, mean reward -7.140, eps 0.02, speed 80.60 f/s\n",
      "Best mean reward updated -7.500 -> -7.140, model saved\n",
      "285182: done 144 games, mean reward -6.780, eps 0.02, speed 80.92 f/s\n",
      "Best mean reward updated -7.140 -> -6.780, model saved\n",
      "288005: done 145 games, mean reward -6.460, eps 0.02, speed 80.78 f/s\n",
      "Best mean reward updated -6.780 -> -6.460, model saved\n",
      "290165: done 146 games, mean reward -6.100, eps 0.02, speed 81.26 f/s\n",
      "Best mean reward updated -6.460 -> -6.100, model saved\n",
      "292472: done 147 games, mean reward -5.740, eps 0.02, speed 81.20 f/s\n",
      "Best mean reward updated -6.100 -> -5.740, model saved\n",
      "294656: done 148 games, mean reward -5.350, eps 0.02, speed 81.23 f/s\n",
      "Best mean reward updated -5.740 -> -5.350, model saved\n",
      "297159: done 149 games, mean reward -5.020, eps 0.02, speed 80.89 f/s\n",
      "Best mean reward updated -5.350 -> -5.020, model saved\n",
      "299613: done 150 games, mean reward -4.670, eps 0.02, speed 81.09 f/s\n",
      "Best mean reward updated -5.020 -> -4.670, model saved\n",
      "302507: done 151 games, mean reward -4.380, eps 0.02, speed 81.48 f/s\n",
      "Best mean reward updated -4.670 -> -4.380, model saved\n",
      "304678: done 152 games, mean reward -4.020, eps 0.02, speed 80.76 f/s\n",
      "Best mean reward updated -4.380 -> -4.020, model saved\n",
      "307168: done 153 games, mean reward -3.660, eps 0.02, speed 81.02 f/s\n",
      "Best mean reward updated -4.020 -> -3.660, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309289: done 154 games, mean reward -3.300, eps 0.02, speed 80.86 f/s\n",
      "Best mean reward updated -3.660 -> -3.300, model saved\n",
      "311853: done 155 games, mean reward -2.970, eps 0.02, speed 80.18 f/s\n",
      "Best mean reward updated -3.300 -> -2.970, model saved\n",
      "314361: done 156 games, mean reward -2.630, eps 0.02, speed 80.41 f/s\n",
      "Best mean reward updated -2.970 -> -2.630, model saved\n",
      "316426: done 157 games, mean reward -2.270, eps 0.02, speed 80.59 f/s\n",
      "Best mean reward updated -2.630 -> -2.270, model saved\n",
      "318684: done 158 games, mean reward -1.910, eps 0.02, speed 80.78 f/s\n",
      "Best mean reward updated -2.270 -> -1.910, model saved\n",
      "321626: done 159 games, mean reward -1.630, eps 0.02, speed 80.87 f/s\n",
      "Best mean reward updated -1.910 -> -1.630, model saved\n",
      "324080: done 160 games, mean reward -1.280, eps 0.02, speed 80.64 f/s\n",
      "Best mean reward updated -1.630 -> -1.280, model saved\n",
      "326316: done 161 games, mean reward -0.930, eps 0.02, speed 81.10 f/s\n",
      "Best mean reward updated -1.280 -> -0.930, model saved\n",
      "328961: done 162 games, mean reward -0.610, eps 0.02, speed 80.48 f/s\n",
      "Best mean reward updated -0.930 -> -0.610, model saved\n",
      "331077: done 163 games, mean reward -0.240, eps 0.02, speed 80.83 f/s\n",
      "Best mean reward updated -0.610 -> -0.240, model saved\n",
      "332967: done 164 games, mean reward 0.140, eps 0.02, speed 80.31 f/s\n",
      "Best mean reward updated -0.240 -> 0.140, model saved\n",
      "335256: done 165 games, mean reward 0.490, eps 0.02, speed 79.91 f/s\n",
      "Best mean reward updated 0.140 -> 0.490, model saved\n",
      "337832: done 166 games, mean reward 0.770, eps 0.02, speed 80.53 f/s\n",
      "Best mean reward updated 0.490 -> 0.770, model saved\n",
      "340316: done 167 games, mean reward 1.120, eps 0.02, speed 80.74 f/s\n",
      "Best mean reward updated 0.770 -> 1.120, model saved\n",
      "342183: done 168 games, mean reward 1.490, eps 0.02, speed 80.81 f/s\n",
      "Best mean reward updated 1.120 -> 1.490, model saved\n",
      "344388: done 169 games, mean reward 1.820, eps 0.02, speed 80.65 f/s\n",
      "Best mean reward updated 1.490 -> 1.820, model saved\n",
      "346375: done 170 games, mean reward 2.140, eps 0.02, speed 79.75 f/s\n",
      "Best mean reward updated 1.820 -> 2.140, model saved\n",
      "348340: done 171 games, mean reward 2.530, eps 0.02, speed 80.53 f/s\n",
      "Best mean reward updated 2.140 -> 2.530, model saved\n",
      "350314: done 172 games, mean reward 2.880, eps 0.02, speed 80.36 f/s\n",
      "Best mean reward updated 2.530 -> 2.880, model saved\n",
      "352363: done 173 games, mean reward 3.210, eps 0.02, speed 80.31 f/s\n",
      "Best mean reward updated 2.880 -> 3.210, model saved\n",
      "354881: done 174 games, mean reward 3.450, eps 0.02, speed 80.21 f/s\n",
      "Best mean reward updated 3.210 -> 3.450, model saved\n",
      "356966: done 175 games, mean reward 3.800, eps 0.02, speed 80.60 f/s\n",
      "Best mean reward updated 3.450 -> 3.800, model saved\n",
      "359161: done 176 games, mean reward 4.160, eps 0.02, speed 80.20 f/s\n",
      "Best mean reward updated 3.800 -> 4.160, model saved\n",
      "361265: done 177 games, mean reward 4.450, eps 0.02, speed 80.24 f/s\n",
      "Best mean reward updated 4.160 -> 4.450, model saved\n",
      "363637: done 178 games, mean reward 4.750, eps 0.02, speed 81.01 f/s\n",
      "Best mean reward updated 4.450 -> 4.750, model saved\n",
      "365727: done 179 games, mean reward 5.080, eps 0.02, speed 80.29 f/s\n",
      "Best mean reward updated 4.750 -> 5.080, model saved\n",
      "367849: done 180 games, mean reward 5.420, eps 0.02, speed 80.31 f/s\n",
      "Best mean reward updated 5.080 -> 5.420, model saved\n",
      "369873: done 181 games, mean reward 5.750, eps 0.02, speed 80.52 f/s\n",
      "Best mean reward updated 5.420 -> 5.750, model saved\n",
      "371899: done 182 games, mean reward 6.120, eps 0.02, speed 80.33 f/s\n",
      "Best mean reward updated 5.750 -> 6.120, model saved\n",
      "373979: done 183 games, mean reward 6.480, eps 0.02, speed 80.97 f/s\n",
      "Best mean reward updated 6.120 -> 6.480, model saved\n",
      "375858: done 184 games, mean reward 6.840, eps 0.02, speed 80.44 f/s\n",
      "Best mean reward updated 6.480 -> 6.840, model saved\n",
      "377755: done 185 games, mean reward 7.220, eps 0.02, speed 81.12 f/s\n",
      "Best mean reward updated 6.840 -> 7.220, model saved\n",
      "379802: done 186 games, mean reward 7.560, eps 0.02, speed 80.21 f/s\n",
      "Best mean reward updated 7.220 -> 7.560, model saved\n",
      "382091: done 187 games, mean reward 7.860, eps 0.02, speed 80.49 f/s\n",
      "Best mean reward updated 7.560 -> 7.860, model saved\n",
      "384152: done 188 games, mean reward 8.180, eps 0.02, speed 80.56 f/s\n",
      "Best mean reward updated 7.860 -> 8.180, model saved\n",
      "385945: done 189 games, mean reward 8.540, eps 0.02, speed 80.61 f/s\n",
      "Best mean reward updated 8.180 -> 8.540, model saved\n",
      "388057: done 190 games, mean reward 8.790, eps 0.02, speed 81.21 f/s\n",
      "Best mean reward updated 8.540 -> 8.790, model saved\n",
      "390153: done 191 games, mean reward 9.010, eps 0.02, speed 81.38 f/s\n",
      "Best mean reward updated 8.790 -> 9.010, model saved\n",
      "392429: done 192 games, mean reward 9.250, eps 0.02, speed 80.37 f/s\n",
      "Best mean reward updated 9.010 -> 9.250, model saved\n",
      "395035: done 193 games, mean reward 9.490, eps 0.02, speed 81.29 f/s\n",
      "Best mean reward updated 9.250 -> 9.490, model saved\n",
      "397286: done 194 games, mean reward 9.760, eps 0.02, speed 80.54 f/s\n",
      "Best mean reward updated 9.490 -> 9.760, model saved\n",
      "399364: done 195 games, mean reward 10.010, eps 0.02, speed 81.08 f/s\n",
      "Best mean reward updated 9.760 -> 10.010, model saved\n",
      "401219: done 196 games, mean reward 10.270, eps 0.02, speed 80.27 f/s\n",
      "Best mean reward updated 10.010 -> 10.270, model saved\n",
      "403311: done 197 games, mean reward 10.530, eps 0.02, speed 80.63 f/s\n",
      "Best mean reward updated 10.270 -> 10.530, model saved\n",
      "405105: done 198 games, mean reward 10.860, eps 0.02, speed 80.76 f/s\n",
      "Best mean reward updated 10.530 -> 10.860, model saved\n",
      "407323: done 199 games, mean reward 11.120, eps 0.02, speed 81.25 f/s\n",
      "Best mean reward updated 10.860 -> 11.120, model saved\n",
      "409334: done 200 games, mean reward 11.380, eps 0.02, speed 79.84 f/s\n",
      "Best mean reward updated 11.120 -> 11.380, model saved\n",
      "411435: done 201 games, mean reward 11.690, eps 0.02, speed 80.32 f/s\n",
      "Best mean reward updated 11.380 -> 11.690, model saved\n",
      "413895: done 202 games, mean reward 11.990, eps 0.02, speed 80.40 f/s\n",
      "Best mean reward updated 11.690 -> 11.990, model saved\n",
      "415764: done 203 games, mean reward 12.290, eps 0.02, speed 81.55 f/s\n",
      "Best mean reward updated 11.990 -> 12.290, model saved\n",
      "417632: done 204 games, mean reward 12.550, eps 0.02, speed 80.85 f/s\n",
      "Best mean reward updated 12.290 -> 12.550, model saved\n",
      "419459: done 205 games, mean reward 12.840, eps 0.02, speed 80.42 f/s\n",
      "Best mean reward updated 12.550 -> 12.840, model saved\n",
      "421345: done 206 games, mean reward 13.110, eps 0.02, speed 81.06 f/s\n",
      "Best mean reward updated 12.840 -> 13.110, model saved\n",
      "423892: done 207 games, mean reward 13.290, eps 0.02, speed 80.24 f/s\n",
      "Best mean reward updated 13.110 -> 13.290, model saved\n",
      "425665: done 208 games, mean reward 13.530, eps 0.02, speed 80.30 f/s\n",
      "Best mean reward updated 13.290 -> 13.530, model saved\n",
      "427610: done 209 games, mean reward 13.720, eps 0.02, speed 81.13 f/s\n",
      "Best mean reward updated 13.530 -> 13.720, model saved\n",
      "429878: done 210 games, mean reward 13.640, eps 0.02, speed 80.28 f/s\n",
      "431607: done 211 games, mean reward 13.810, eps 0.02, speed 80.16 f/s\n",
      "Best mean reward updated 13.720 -> 13.810, model saved\n",
      "433479: done 212 games, mean reward 14.010, eps 0.02, speed 78.75 f/s\n",
      "Best mean reward updated 13.810 -> 14.010, model saved\n",
      "435191: done 213 games, mean reward 14.210, eps 0.02, speed 79.59 f/s\n",
      "Best mean reward updated 14.010 -> 14.210, model saved\n",
      "436861: done 214 games, mean reward 14.380, eps 0.02, speed 80.55 f/s\n",
      "Best mean reward updated 14.210 -> 14.380, model saved\n",
      "438574: done 215 games, mean reward 14.490, eps 0.02, speed 81.65 f/s\n",
      "Best mean reward updated 14.380 -> 14.490, model saved\n",
      "440733: done 216 games, mean reward 14.570, eps 0.02, speed 79.70 f/s\n",
      "Best mean reward updated 14.490 -> 14.570, model saved\n",
      "442698: done 217 games, mean reward 14.670, eps 0.02, speed 79.40 f/s\n",
      "Best mean reward updated 14.570 -> 14.670, model saved\n",
      "444622: done 218 games, mean reward 14.840, eps 0.02, speed 80.89 f/s\n",
      "Best mean reward updated 14.670 -> 14.840, model saved\n",
      "446904: done 219 games, mean reward 15.010, eps 0.02, speed 79.32 f/s\n",
      "Best mean reward updated 14.840 -> 15.010, model saved\n",
      "448662: done 220 games, mean reward 15.160, eps 0.02, speed 79.36 f/s\n",
      "Best mean reward updated 15.010 -> 15.160, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450834: done 221 games, mean reward 15.250, eps 0.02, speed 79.17 f/s\n",
      "Best mean reward updated 15.160 -> 15.250, model saved\n",
      "452849: done 222 games, mean reward 15.350, eps 0.02, speed 79.19 f/s\n",
      "Best mean reward updated 15.250 -> 15.350, model saved\n",
      "454624: done 223 games, mean reward 15.430, eps 0.02, speed 79.58 f/s\n",
      "Best mean reward updated 15.350 -> 15.430, model saved\n",
      "456518: done 224 games, mean reward 15.400, eps 0.02, speed 81.13 f/s\n",
      "458257: done 225 games, mean reward 15.610, eps 0.02, speed 81.32 f/s\n",
      "Best mean reward updated 15.430 -> 15.610, model saved\n",
      "460426: done 226 games, mean reward 15.600, eps 0.02, speed 81.60 f/s\n",
      "462826: done 227 games, mean reward 15.720, eps 0.02, speed 80.65 f/s\n",
      "Best mean reward updated 15.610 -> 15.720, model saved\n",
      "464522: done 228 games, mean reward 15.920, eps 0.02, speed 80.18 f/s\n",
      "Best mean reward updated 15.720 -> 15.920, model saved\n",
      "466364: done 229 games, mean reward 16.010, eps 0.02, speed 79.96 f/s\n",
      "Best mean reward updated 15.920 -> 16.010, model saved\n",
      "468394: done 230 games, mean reward 16.120, eps 0.02, speed 79.28 f/s\n",
      "Best mean reward updated 16.010 -> 16.120, model saved\n",
      "470252: done 231 games, mean reward 16.180, eps 0.02, speed 78.74 f/s\n",
      "Best mean reward updated 16.120 -> 16.180, model saved\n",
      "472023: done 232 games, mean reward 16.330, eps 0.02, speed 77.58 f/s\n",
      "Best mean reward updated 16.180 -> 16.330, model saved\n",
      "473928: done 233 games, mean reward 16.420, eps 0.02, speed 78.81 f/s\n",
      "Best mean reward updated 16.330 -> 16.420, model saved\n",
      "475624: done 234 games, mean reward 16.500, eps 0.02, speed 78.51 f/s\n",
      "Best mean reward updated 16.420 -> 16.500, model saved\n",
      "477592: done 235 games, mean reward 16.540, eps 0.02, speed 78.95 f/s\n",
      "Best mean reward updated 16.500 -> 16.540, model saved\n",
      "479291: done 236 games, mean reward 16.610, eps 0.02, speed 78.50 f/s\n",
      "Best mean reward updated 16.540 -> 16.610, model saved\n",
      "481196: done 237 games, mean reward 16.690, eps 0.02, speed 80.08 f/s\n",
      "Best mean reward updated 16.610 -> 16.690, model saved\n",
      "483041: done 238 games, mean reward 16.760, eps 0.02, speed 78.07 f/s\n",
      "Best mean reward updated 16.690 -> 16.760, model saved\n",
      "484876: done 239 games, mean reward 16.780, eps 0.02, speed 77.90 f/s\n",
      "Best mean reward updated 16.760 -> 16.780, model saved\n",
      "486699: done 240 games, mean reward 16.860, eps 0.02, speed 77.73 f/s\n",
      "Best mean reward updated 16.780 -> 16.860, model saved\n",
      "488577: done 241 games, mean reward 16.910, eps 0.02, speed 79.39 f/s\n",
      "Best mean reward updated 16.860 -> 16.910, model saved\n",
      "490455: done 242 games, mean reward 16.990, eps 0.02, speed 79.43 f/s\n",
      "Best mean reward updated 16.910 -> 16.990, model saved\n",
      "492123: done 243 games, mean reward 17.040, eps 0.02, speed 79.81 f/s\n",
      "Best mean reward updated 16.990 -> 17.040, model saved\n",
      "493876: done 244 games, mean reward 17.100, eps 0.02, speed 80.56 f/s\n",
      "Best mean reward updated 17.040 -> 17.100, model saved\n",
      "495643: done 245 games, mean reward 17.160, eps 0.02, speed 77.53 f/s\n",
      "Best mean reward updated 17.100 -> 17.160, model saved\n",
      "497409: done 246 games, mean reward 17.170, eps 0.02, speed 78.44 f/s\n",
      "Best mean reward updated 17.160 -> 17.170, model saved\n",
      "499561: done 247 games, mean reward 17.180, eps 0.02, speed 78.02 f/s\n",
      "Best mean reward updated 17.170 -> 17.180, model saved\n",
      "501294: done 248 games, mean reward 17.190, eps 0.02, speed 77.83 f/s\n",
      "Best mean reward updated 17.180 -> 17.190, model saved\n",
      "503262: done 249 games, mean reward 17.180, eps 0.02, speed 78.24 f/s\n",
      "505063: done 250 games, mean reward 17.190, eps 0.02, speed 78.30 f/s\n",
      "506983: done 251 games, mean reward 17.260, eps 0.02, speed 78.85 f/s\n",
      "Best mean reward updated 17.190 -> 17.260, model saved\n",
      "508681: done 252 games, mean reward 17.290, eps 0.02, speed 78.26 f/s\n",
      "Best mean reward updated 17.260 -> 17.290, model saved\n",
      "510541: done 253 games, mean reward 17.320, eps 0.02, speed 79.39 f/s\n",
      "Best mean reward updated 17.290 -> 17.320, model saved\n",
      "512775: done 254 games, mean reward 17.310, eps 0.02, speed 78.40 f/s\n",
      "514708: done 255 games, mean reward 17.350, eps 0.02, speed 78.56 f/s\n",
      "Best mean reward updated 17.320 -> 17.350, model saved\n",
      "516617: done 256 games, mean reward 17.380, eps 0.02, speed 78.30 f/s\n",
      "Best mean reward updated 17.350 -> 17.380, model saved\n",
      "518727: done 257 games, mean reward 17.380, eps 0.02, speed 78.11 f/s\n",
      "520803: done 258 games, mean reward 17.350, eps 0.02, speed 77.48 f/s\n",
      "522851: done 259 games, mean reward 17.450, eps 0.02, speed 79.74 f/s\n",
      "Best mean reward updated 17.380 -> 17.450, model saved\n",
      "524629: done 260 games, mean reward 17.490, eps 0.02, speed 77.91 f/s\n",
      "Best mean reward updated 17.450 -> 17.490, model saved\n",
      "526473: done 261 games, mean reward 17.520, eps 0.02, speed 78.06 f/s\n",
      "Best mean reward updated 17.490 -> 17.520, model saved\n",
      "528387: done 262 games, mean reward 17.550, eps 0.02, speed 80.71 f/s\n",
      "Best mean reward updated 17.520 -> 17.550, model saved\n",
      "530194: done 263 games, mean reward 17.580, eps 0.02, speed 79.82 f/s\n",
      "Best mean reward updated 17.550 -> 17.580, model saved\n",
      "532033: done 264 games, mean reward 17.570, eps 0.02, speed 79.57 f/s\n",
      "533938: done 265 games, mean reward 17.560, eps 0.02, speed 79.72 f/s\n",
      "536015: done 266 games, mean reward 17.570, eps 0.02, speed 79.59 f/s\n",
      "537682: done 267 games, mean reward 17.610, eps 0.02, speed 79.29 f/s\n",
      "Best mean reward updated 17.580 -> 17.610, model saved\n",
      "539475: done 268 games, mean reward 17.590, eps 0.02, speed 79.10 f/s\n",
      "541223: done 269 games, mean reward 17.610, eps 0.02, speed 79.65 f/s\n",
      "542988: done 270 games, mean reward 17.630, eps 0.02, speed 81.04 f/s\n",
      "Best mean reward updated 17.610 -> 17.630, model saved\n",
      "544828: done 271 games, mean reward 17.620, eps 0.02, speed 80.81 f/s\n",
      "546600: done 272 games, mean reward 17.630, eps 0.02, speed 79.59 f/s\n",
      "548325: done 273 games, mean reward 17.670, eps 0.02, speed 78.71 f/s\n",
      "Best mean reward updated 17.630 -> 17.670, model saved\n",
      "550348: done 274 games, mean reward 17.710, eps 0.02, speed 78.92 f/s\n",
      "Best mean reward updated 17.670 -> 17.710, model saved\n",
      "552382: done 275 games, mean reward 17.700, eps 0.02, speed 80.50 f/s\n",
      "554112: done 276 games, mean reward 17.750, eps 0.02, speed 81.13 f/s\n",
      "Best mean reward updated 17.710 -> 17.750, model saved\n",
      "556019: done 277 games, mean reward 17.760, eps 0.02, speed 81.09 f/s\n",
      "Best mean reward updated 17.750 -> 17.760, model saved\n",
      "557778: done 278 games, mean reward 17.840, eps 0.02, speed 80.46 f/s\n",
      "Best mean reward updated 17.760 -> 17.840, model saved\n",
      "559770: done 279 games, mean reward 17.860, eps 0.02, speed 79.78 f/s\n",
      "Best mean reward updated 17.840 -> 17.860, model saved\n",
      "561641: done 280 games, mean reward 17.860, eps 0.02, speed 81.31 f/s\n",
      "563392: done 281 games, mean reward 17.860, eps 0.02, speed 80.44 f/s\n",
      "565432: done 282 games, mean reward 17.840, eps 0.02, speed 80.98 f/s\n",
      "567534: done 283 games, mean reward 17.830, eps 0.02, speed 80.28 f/s\n",
      "569287: done 284 games, mean reward 17.840, eps 0.02, speed 79.39 f/s\n",
      "571020: done 285 games, mean reward 17.840, eps 0.02, speed 78.28 f/s\n",
      "572904: done 286 games, mean reward 17.850, eps 0.02, speed 79.50 f/s\n",
      "574984: done 287 games, mean reward 17.840, eps 0.02, speed 79.28 f/s\n",
      "576849: done 288 games, mean reward 17.850, eps 0.02, speed 79.40 f/s\n",
      "578765: done 289 games, mean reward 17.830, eps 0.02, speed 79.32 f/s\n",
      "580785: done 290 games, mean reward 17.830, eps 0.02, speed 79.12 f/s\n",
      "582727: done 291 games, mean reward 17.840, eps 0.02, speed 79.88 f/s\n",
      "584461: done 292 games, mean reward 17.880, eps 0.02, speed 79.72 f/s\n",
      "Best mean reward updated 17.860 -> 17.880, model saved\n",
      "586459: done 293 games, mean reward 17.920, eps 0.02, speed 79.55 f/s\n",
      "Best mean reward updated 17.880 -> 17.920, model saved\n",
      "588590: done 294 games, mean reward 17.940, eps 0.02, speed 79.50 f/s\n",
      "Best mean reward updated 17.920 -> 17.940, model saved\n",
      "590505: done 295 games, mean reward 17.970, eps 0.02, speed 80.46 f/s\n",
      "Best mean reward updated 17.940 -> 17.970, model saved\n",
      "592414: done 296 games, mean reward 17.960, eps 0.02, speed 79.88 f/s\n",
      "594214: done 297 games, mean reward 17.980, eps 0.02, speed 79.93 f/s\n",
      "Best mean reward updated 17.970 -> 17.980, model saved\n",
      "596390: done 298 games, mean reward 17.940, eps 0.02, speed 79.68 f/s\n",
      "598160: done 299 games, mean reward 17.970, eps 0.02, speed 79.43 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600198: done 300 games, mean reward 17.970, eps 0.02, speed 79.14 f/s\n",
      "602159: done 301 games, mean reward 17.960, eps 0.02, speed 79.13 f/s\n",
      "603963: done 302 games, mean reward 18.000, eps 0.02, speed 79.70 f/s\n",
      "Best mean reward updated 17.980 -> 18.000, model saved\n",
      "605792: done 303 games, mean reward 18.000, eps 0.02, speed 79.38 f/s\n",
      "607584: done 304 games, mean reward 18.010, eps 0.02, speed 80.70 f/s\n",
      "Best mean reward updated 18.000 -> 18.010, model saved\n",
      "609364: done 305 games, mean reward 18.000, eps 0.02, speed 79.93 f/s\n",
      "611308: done 306 games, mean reward 18.010, eps 0.02, speed 79.48 f/s\n",
      "612982: done 307 games, mean reward 18.070, eps 0.02, speed 79.19 f/s\n",
      "Best mean reward updated 18.010 -> 18.070, model saved\n",
      "614917: done 308 games, mean reward 18.040, eps 0.02, speed 79.45 f/s\n",
      "616561: done 309 games, mean reward 18.080, eps 0.02, speed 80.04 f/s\n",
      "Best mean reward updated 18.070 -> 18.080, model saved\n",
      "618323: done 310 games, mean reward 18.340, eps 0.02, speed 79.34 f/s\n",
      "Best mean reward updated 18.080 -> 18.340, model saved\n",
      "620098: done 311 games, mean reward 18.320, eps 0.02, speed 79.07 f/s\n",
      "621770: done 312 games, mean reward 18.340, eps 0.02, speed 79.14 f/s\n",
      "623485: done 313 games, mean reward 18.330, eps 0.02, speed 78.93 f/s\n",
      "625154: done 314 games, mean reward 18.330, eps 0.02, speed 79.63 f/s\n",
      "626998: done 315 games, mean reward 18.320, eps 0.02, speed 79.32 f/s\n",
      "629041: done 316 games, mean reward 18.330, eps 0.02, speed 79.04 f/s\n",
      "630681: done 317 games, mean reward 18.360, eps 0.02, speed 79.06 f/s\n",
      "Best mean reward updated 18.340 -> 18.360, model saved\n",
      "632476: done 318 games, mean reward 18.370, eps 0.02, speed 80.23 f/s\n",
      "Best mean reward updated 18.360 -> 18.370, model saved\n",
      "634147: done 319 games, mean reward 18.430, eps 0.02, speed 79.66 f/s\n",
      "Best mean reward updated 18.370 -> 18.430, model saved\n",
      "635966: done 320 games, mean reward 18.430, eps 0.02, speed 79.22 f/s\n",
      "637845: done 321 games, mean reward 18.440, eps 0.02, speed 79.01 f/s\n",
      "Best mean reward updated 18.430 -> 18.440, model saved\n",
      "639660: done 322 games, mean reward 18.450, eps 0.02, speed 78.55 f/s\n",
      "Best mean reward updated 18.440 -> 18.450, model saved\n",
      "641431: done 323 games, mean reward 18.440, eps 0.02, speed 78.49 f/s\n",
      "643106: done 324 games, mean reward 18.460, eps 0.02, speed 78.08 f/s\n",
      "Best mean reward updated 18.450 -> 18.460, model saved\n",
      "644779: done 325 games, mean reward 18.470, eps 0.02, speed 79.89 f/s\n",
      "Best mean reward updated 18.460 -> 18.470, model saved\n",
      "646604: done 326 games, mean reward 18.480, eps 0.02, speed 79.10 f/s\n",
      "Best mean reward updated 18.470 -> 18.480, model saved\n",
      "648242: done 327 games, mean reward 18.530, eps 0.02, speed 79.58 f/s\n",
      "Best mean reward updated 18.480 -> 18.530, model saved\n",
      "650010: done 328 games, mean reward 18.510, eps 0.02, speed 79.41 f/s\n",
      "652075: done 329 games, mean reward 18.480, eps 0.02, speed 79.33 f/s\n",
      "654086: done 330 games, mean reward 18.470, eps 0.02, speed 78.94 f/s\n",
      "655785: done 331 games, mean reward 18.490, eps 0.02, speed 79.67 f/s\n",
      "657598: done 332 games, mean reward 18.470, eps 0.02, speed 79.54 f/s\n",
      "659529: done 333 games, mean reward 18.470, eps 0.02, speed 78.93 f/s\n",
      "661202: done 334 games, mean reward 18.460, eps 0.02, speed 79.13 f/s\n",
      "662998: done 335 games, mean reward 18.540, eps 0.02, speed 79.37 f/s\n",
      "Best mean reward updated 18.530 -> 18.540, model saved\n",
      "664887: done 336 games, mean reward 18.530, eps 0.02, speed 78.92 f/s\n",
      "666876: done 337 games, mean reward 18.520, eps 0.02, speed 78.06 f/s\n",
      "668742: done 338 games, mean reward 18.520, eps 0.02, speed 77.60 f/s\n",
      "670528: done 339 games, mean reward 18.530, eps 0.02, speed 77.68 f/s\n",
      "672200: done 340 games, mean reward 18.530, eps 0.02, speed 78.93 f/s\n",
      "674531: done 341 games, mean reward 18.510, eps 0.02, speed 80.21 f/s\n",
      "676344: done 342 games, mean reward 18.520, eps 0.02, speed 80.81 f/s\n",
      "677984: done 343 games, mean reward 18.530, eps 0.02, speed 78.75 f/s\n",
      "679973: done 344 games, mean reward 18.500, eps 0.02, speed 78.78 f/s\n",
      "681707: done 345 games, mean reward 18.510, eps 0.02, speed 79.39 f/s\n",
      "683344: done 346 games, mean reward 18.530, eps 0.02, speed 79.18 f/s\n",
      "685151: done 347 games, mean reward 18.540, eps 0.02, speed 79.72 f/s\n",
      "687100: done 348 games, mean reward 18.510, eps 0.02, speed 79.96 f/s\n",
      "688738: done 349 games, mean reward 18.580, eps 0.02, speed 78.97 f/s\n",
      "Best mean reward updated 18.540 -> 18.580, model saved\n",
      "690438: done 350 games, mean reward 18.590, eps 0.02, speed 79.28 f/s\n",
      "Best mean reward updated 18.580 -> 18.590, model saved\n",
      "692309: done 351 games, mean reward 18.620, eps 0.02, speed 79.26 f/s\n",
      "Best mean reward updated 18.590 -> 18.620, model saved\n",
      "694207: done 352 games, mean reward 18.590, eps 0.02, speed 79.54 f/s\n",
      "695987: done 353 games, mean reward 18.600, eps 0.02, speed 79.57 f/s\n",
      "697735: done 354 games, mean reward 18.630, eps 0.02, speed 79.21 f/s\n",
      "Best mean reward updated 18.620 -> 18.630, model saved\n",
      "699485: done 355 games, mean reward 18.650, eps 0.02, speed 78.96 f/s\n",
      "Best mean reward updated 18.630 -> 18.650, model saved\n",
      "701247: done 356 games, mean reward 18.670, eps 0.02, speed 79.43 f/s\n",
      "Best mean reward updated 18.650 -> 18.670, model saved\n",
      "703162: done 357 games, mean reward 18.700, eps 0.02, speed 79.78 f/s\n",
      "Best mean reward updated 18.670 -> 18.700, model saved\n",
      "704799: done 358 games, mean reward 18.760, eps 0.02, speed 79.47 f/s\n",
      "Best mean reward updated 18.700 -> 18.760, model saved\n",
      "706585: done 359 games, mean reward 18.780, eps 0.02, speed 79.29 f/s\n",
      "Best mean reward updated 18.760 -> 18.780, model saved\n",
      "708221: done 360 games, mean reward 18.790, eps 0.02, speed 79.85 f/s\n",
      "Best mean reward updated 18.780 -> 18.790, model saved\n",
      "709913: done 361 games, mean reward 18.800, eps 0.02, speed 80.74 f/s\n",
      "Best mean reward updated 18.790 -> 18.800, model saved\n",
      "711672: done 362 games, mean reward 18.830, eps 0.02, speed 82.35 f/s\n",
      "Best mean reward updated 18.800 -> 18.830, model saved\n",
      "713325: done 363 games, mean reward 18.840, eps 0.02, speed 80.83 f/s\n",
      "Best mean reward updated 18.830 -> 18.840, model saved\n",
      "715035: done 364 games, mean reward 18.860, eps 0.02, speed 81.65 f/s\n",
      "Best mean reward updated 18.840 -> 18.860, model saved\n",
      "716846: done 365 games, mean reward 18.880, eps 0.02, speed 78.85 f/s\n",
      "Best mean reward updated 18.860 -> 18.880, model saved\n",
      "718482: done 366 games, mean reward 18.950, eps 0.02, speed 78.36 f/s\n",
      "Best mean reward updated 18.880 -> 18.950, model saved\n",
      "720601: done 367 games, mean reward 18.930, eps 0.02, speed 78.41 f/s\n",
      "722385: done 368 games, mean reward 18.930, eps 0.02, speed 79.58 f/s\n",
      "724023: done 369 games, mean reward 18.950, eps 0.02, speed 80.36 f/s\n",
      "725658: done 370 games, mean reward 18.970, eps 0.02, speed 79.53 f/s\n",
      "Best mean reward updated 18.950 -> 18.970, model saved\n",
      "727294: done 371 games, mean reward 19.000, eps 0.02, speed 79.86 f/s\n",
      "Best mean reward updated 18.970 -> 19.000, model saved\n",
      "728982: done 372 games, mean reward 19.000, eps 0.02, speed 80.91 f/s\n",
      "730722: done 373 games, mean reward 18.990, eps 0.02, speed 79.75 f/s\n",
      "732988: done 374 games, mean reward 18.940, eps 0.02, speed 78.86 f/s\n",
      "734625: done 375 games, mean reward 18.970, eps 0.02, speed 79.10 f/s\n",
      "736325: done 376 games, mean reward 18.970, eps 0.02, speed 81.73 f/s\n",
      "737962: done 377 games, mean reward 19.000, eps 0.02, speed 80.94 f/s\n",
      "739729: done 378 games, mean reward 18.990, eps 0.02, speed 80.60 f/s\n",
      "741661: done 379 games, mean reward 18.980, eps 0.02, speed 79.05 f/s\n",
      "743299: done 380 games, mean reward 19.020, eps 0.02, speed 78.42 f/s\n",
      "Best mean reward updated 19.000 -> 19.020, model saved\n",
      "745328: done 381 games, mean reward 18.990, eps 0.02, speed 78.51 f/s\n",
      "746965: done 382 games, mean reward 19.040, eps 0.02, speed 80.55 f/s\n",
      "Best mean reward updated 19.020 -> 19.040, model saved\n",
      "748656: done 383 games, mean reward 19.090, eps 0.02, speed 82.51 f/s\n",
      "Best mean reward updated 19.040 -> 19.090, model saved\n",
      "750476: done 384 games, mean reward 19.090, eps 0.02, speed 82.76 f/s\n",
      "752245: done 385 games, mean reward 19.080, eps 0.02, speed 82.57 f/s\n",
      "753880: done 386 games, mean reward 19.110, eps 0.02, speed 80.72 f/s\n",
      "Best mean reward updated 19.090 -> 19.110, model saved\n",
      "755516: done 387 games, mean reward 19.160, eps 0.02, speed 80.66 f/s\n",
      "Best mean reward updated 19.110 -> 19.160, model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "757325: done 388 games, mean reward 19.160, eps 0.02, speed 80.27 f/s\n",
      "759312: done 389 games, mean reward 19.130, eps 0.02, speed 77.45 f/s\n",
      "761236: done 390 games, mean reward 19.140, eps 0.02, speed 81.31 f/s\n",
      "762941: done 391 games, mean reward 19.160, eps 0.02, speed 80.28 f/s\n",
      "764630: done 392 games, mean reward 19.160, eps 0.02, speed 78.58 f/s\n",
      "766944: done 393 games, mean reward 19.110, eps 0.02, speed 79.17 f/s\n",
      "768689: done 394 games, mean reward 19.140, eps 0.02, speed 79.44 f/s\n",
      "770636: done 395 games, mean reward 19.140, eps 0.02, speed 81.80 f/s\n",
      "772433: done 396 games, mean reward 19.150, eps 0.02, speed 81.23 f/s\n",
      "774205: done 397 games, mean reward 19.150, eps 0.02, speed 79.54 f/s\n",
      "775893: done 398 games, mean reward 19.190, eps 0.02, speed 81.12 f/s\n",
      "Best mean reward updated 19.160 -> 19.190, model saved\n",
      "777804: done 399 games, mean reward 19.170, eps 0.02, speed 81.09 f/s\n",
      "779439: done 400 games, mean reward 19.200, eps 0.02, speed 78.77 f/s\n",
      "Best mean reward updated 19.190 -> 19.200, model saved\n",
      "781308: done 401 games, mean reward 19.210, eps 0.02, speed 78.89 f/s\n",
      "Best mean reward updated 19.200 -> 19.210, model saved\n",
      "783025: done 402 games, mean reward 19.220, eps 0.02, speed 78.95 f/s\n",
      "Best mean reward updated 19.210 -> 19.220, model saved\n",
      "785035: done 403 games, mean reward 19.210, eps 0.02, speed 80.90 f/s\n",
      "786701: done 404 games, mean reward 19.220, eps 0.02, speed 80.50 f/s\n",
      "788421: done 405 games, mean reward 19.240, eps 0.02, speed 81.02 f/s\n",
      "Best mean reward updated 19.220 -> 19.240, model saved\n",
      "790059: done 406 games, mean reward 19.270, eps 0.02, speed 81.13 f/s\n",
      "Best mean reward updated 19.240 -> 19.270, model saved\n",
      "791712: done 407 games, mean reward 19.280, eps 0.02, speed 81.52 f/s\n",
      "Best mean reward updated 19.270 -> 19.280, model saved\n",
      "793514: done 408 games, mean reward 19.300, eps 0.02, speed 80.86 f/s\n",
      "Best mean reward updated 19.280 -> 19.300, model saved\n",
      "795360: done 409 games, mean reward 19.270, eps 0.02, speed 80.45 f/s\n",
      "797122: done 410 games, mean reward 19.260, eps 0.02, speed 80.95 f/s\n",
      "798757: done 411 games, mean reward 19.290, eps 0.02, speed 80.73 f/s\n",
      "800571: done 412 games, mean reward 19.280, eps 0.02, speed 80.65 f/s\n",
      "802329: done 413 games, mean reward 19.270, eps 0.02, speed 80.33 f/s\n",
      "804107: done 414 games, mean reward 19.250, eps 0.02, speed 82.83 f/s\n",
      "805762: done 415 games, mean reward 19.260, eps 0.02, speed 83.82 f/s\n",
      "807582: done 416 games, mean reward 19.300, eps 0.02, speed 82.77 f/s\n",
      "809217: done 417 games, mean reward 19.300, eps 0.02, speed 82.66 f/s\n",
      "810905: done 418 games, mean reward 19.310, eps 0.02, speed 81.74 f/s\n",
      "Best mean reward updated 19.300 -> 19.310, model saved\n",
      "812540: done 419 games, mean reward 19.320, eps 0.02, speed 82.59 f/s\n",
      "Best mean reward updated 19.310 -> 19.320, model saved\n",
      "814243: done 420 games, mean reward 19.320, eps 0.02, speed 82.21 f/s\n",
      "815938: done 421 games, mean reward 19.350, eps 0.02, speed 82.09 f/s\n",
      "Best mean reward updated 19.320 -> 19.350, model saved\n",
      "817894: done 422 games, mean reward 19.340, eps 0.02, speed 82.60 f/s\n",
      "819645: done 423 games, mean reward 19.340, eps 0.02, speed 81.98 f/s\n",
      "821426: done 424 games, mean reward 19.330, eps 0.02, speed 82.49 f/s\n",
      "823078: done 425 games, mean reward 19.340, eps 0.02, speed 82.15 f/s\n",
      "824984: done 426 games, mean reward 19.340, eps 0.02, speed 81.71 f/s\n",
      "826811: done 427 games, mean reward 19.320, eps 0.02, speed 81.52 f/s\n",
      "828836: done 428 games, mean reward 19.290, eps 0.02, speed 81.90 f/s\n",
      "830471: done 429 games, mean reward 19.340, eps 0.02, speed 82.22 f/s\n",
      "832202: done 430 games, mean reward 19.380, eps 0.02, speed 81.80 f/s\n",
      "Best mean reward updated 19.350 -> 19.380, model saved\n",
      "833836: done 431 games, mean reward 19.390, eps 0.02, speed 82.07 f/s\n",
      "Best mean reward updated 19.380 -> 19.390, model saved\n",
      "835532: done 432 games, mean reward 19.400, eps 0.02, speed 81.86 f/s\n",
      "Best mean reward updated 19.390 -> 19.400, model saved\n",
      "837259: done 433 games, mean reward 19.430, eps 0.02, speed 81.79 f/s\n",
      "Best mean reward updated 19.400 -> 19.430, model saved\n",
      "839051: done 434 games, mean reward 19.420, eps 0.02, speed 82.35 f/s\n",
      "840803: done 435 games, mean reward 19.410, eps 0.02, speed 82.81 f/s\n",
      "842488: done 436 games, mean reward 19.420, eps 0.02, speed 82.14 f/s\n",
      "844507: done 437 games, mean reward 19.410, eps 0.02, speed 83.36 f/s\n",
      "846194: done 438 games, mean reward 19.420, eps 0.02, speed 81.74 f/s\n",
      "848264: done 439 games, mean reward 19.420, eps 0.02, speed 81.87 f/s\n",
      "850057: done 440 games, mean reward 19.410, eps 0.02, speed 82.49 f/s\n",
      "851788: done 441 games, mean reward 19.450, eps 0.02, speed 82.88 f/s\n",
      "Best mean reward updated 19.430 -> 19.450, model saved\n",
      "853813: done 442 games, mean reward 19.420, eps 0.02, speed 82.02 f/s\n",
      "855450: done 443 games, mean reward 19.420, eps 0.02, speed 82.14 f/s\n",
      "857155: done 444 games, mean reward 19.440, eps 0.02, speed 82.72 f/s\n",
      "859011: done 445 games, mean reward 19.420, eps 0.02, speed 81.75 f/s\n",
      "860681: done 446 games, mean reward 19.410, eps 0.02, speed 81.68 f/s\n",
      "862315: done 447 games, mean reward 19.440, eps 0.02, speed 82.22 f/s\n",
      "864064: done 448 games, mean reward 19.470, eps 0.02, speed 80.19 f/s\n",
      "Best mean reward updated 19.450 -> 19.470, model saved\n",
      "865822: done 449 games, mean reward 19.460, eps 0.02, speed 80.49 f/s\n",
      "867760: done 450 games, mean reward 19.460, eps 0.02, speed 81.41 f/s\n",
      "869445: done 451 games, mean reward 19.460, eps 0.02, speed 79.78 f/s\n",
      "871253: done 452 games, mean reward 19.480, eps 0.02, speed 81.46 f/s\n",
      "Best mean reward updated 19.470 -> 19.480, model saved\n",
      "873055: done 453 games, mean reward 19.470, eps 0.02, speed 80.50 f/s\n",
      "874917: done 454 games, mean reward 19.460, eps 0.02, speed 80.01 f/s\n",
      "876725: done 455 games, mean reward 19.460, eps 0.02, speed 80.24 f/s\n",
      "878360: done 456 games, mean reward 19.470, eps 0.02, speed 80.53 f/s\n",
      "880105: done 457 games, mean reward 19.470, eps 0.02, speed 79.86 f/s\n",
      "881740: done 458 games, mean reward 19.470, eps 0.02, speed 79.87 f/s\n",
      "883769: done 459 games, mean reward 19.460, eps 0.02, speed 82.04 f/s\n",
      "885655: done 460 games, mean reward 19.430, eps 0.02, speed 80.52 f/s\n",
      "887356: done 461 games, mean reward 19.430, eps 0.02, speed 81.35 f/s\n",
      "889199: done 462 games, mean reward 19.410, eps 0.02, speed 82.84 f/s\n",
      "890906: done 463 games, mean reward 19.400, eps 0.02, speed 79.02 f/s\n",
      "892593: done 464 games, mean reward 19.400, eps 0.02, speed 81.21 f/s\n",
      "894225: done 465 games, mean reward 19.420, eps 0.02, speed 78.15 f/s\n",
      "896160: done 466 games, mean reward 19.380, eps 0.02, speed 78.55 f/s\n",
      "897827: done 467 games, mean reward 19.400, eps 0.02, speed 79.36 f/s\n",
      "899511: done 468 games, mean reward 19.410, eps 0.02, speed 79.02 f/s\n",
      "901237: done 469 games, mean reward 19.400, eps 0.02, speed 79.04 f/s\n",
      "902924: done 470 games, mean reward 19.390, eps 0.02, speed 79.08 f/s\n",
      "904629: done 471 games, mean reward 19.380, eps 0.02, speed 79.66 f/s\n",
      "906335: done 472 games, mean reward 19.380, eps 0.02, speed 78.90 f/s\n",
      "908022: done 473 games, mean reward 19.390, eps 0.02, speed 78.96 f/s\n",
      "909902: done 474 games, mean reward 19.430, eps 0.02, speed 79.33 f/s\n",
      "911779: done 475 games, mean reward 19.400, eps 0.02, speed 79.29 f/s\n",
      "913466: done 476 games, mean reward 19.400, eps 0.02, speed 78.55 f/s\n",
      "915475: done 477 games, mean reward 19.350, eps 0.02, speed 78.89 f/s\n",
      "917297: done 478 games, mean reward 19.350, eps 0.02, speed 78.67 f/s\n",
      "918928: done 479 games, mean reward 19.380, eps 0.02, speed 79.12 f/s\n",
      "920656: done 480 games, mean reward 19.370, eps 0.02, speed 79.67 f/s\n",
      "922586: done 481 games, mean reward 19.400, eps 0.02, speed 79.18 f/s\n",
      "924404: done 482 games, mean reward 19.370, eps 0.02, speed 81.51 f/s\n",
      "926161: done 483 games, mean reward 19.370, eps 0.02, speed 81.65 f/s\n",
      "927793: done 484 games, mean reward 19.390, eps 0.02, speed 79.08 f/s\n",
      "929537: done 485 games, mean reward 19.400, eps 0.02, speed 79.50 f/s\n",
      "931233: done 486 games, mean reward 19.390, eps 0.02, speed 79.37 f/s\n",
      "932868: done 487 games, mean reward 19.390, eps 0.02, speed 78.54 f/s\n",
      "934583: done 488 games, mean reward 19.410, eps 0.02, speed 79.10 f/s\n",
      "936217: done 489 games, mean reward 19.460, eps 0.02, speed 78.40 f/s\n",
      "937903: done 490 games, mean reward 19.480, eps 0.02, speed 78.51 f/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "939572: done 491 games, mean reward 19.480, eps 0.02, speed 79.40 f/s\n",
      "941330: done 492 games, mean reward 19.480, eps 0.02, speed 79.15 f/s\n",
      "943176: done 493 games, mean reward 19.530, eps 0.02, speed 78.88 f/s\n",
      "Best mean reward updated 19.480 -> 19.530, model saved\n",
      "Solved in 943176 frames!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "env = make_env(DEFAULT_ENV_NAME)\n",
    "\n",
    "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_mean_reward = None\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
    "\n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, eps %.2f, speed %.2f f/s\" % (\n",
    "            frame_idx, len(total_rewards), mean_reward, epsilon,\n",
    "            speed\n",
    "        ))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "            torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "            if best_mean_reward is not None:\n",
    "                print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "            best_mean_reward = mean_reward\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
