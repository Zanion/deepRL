{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "Step = namedtuple('Step', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n,), dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]).to(torch.device('cuda'))\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.cpu().data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(Step(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: loss=1.388, reward_mean=0.010, reward_bound=0.000, batch=1\n",
      "2: loss=1.380, reward_mean=0.000, reward_bound=0.000, batch=1\n",
      "3: loss=1.373, reward_mean=0.000, reward_bound=0.000, batch=1\n",
      "4: loss=1.370, reward_mean=0.010, reward_bound=0.000, batch=2\n",
      "5: loss=1.372, reward_mean=0.010, reward_bound=0.000, batch=3\n",
      "6: loss=1.366, reward_mean=0.010, reward_bound=0.000, batch=4\n",
      "7: loss=1.378, reward_mean=0.020, reward_bound=0.000, batch=6\n",
      "8: loss=1.380, reward_mean=0.010, reward_bound=0.000, batch=7\n",
      "9: loss=1.379, reward_mean=0.000, reward_bound=0.000, batch=7\n",
      "10: loss=1.378, reward_mean=0.000, reward_bound=0.000, batch=7\n",
      "11: loss=1.379, reward_mean=0.010, reward_bound=0.000, batch=8\n",
      "12: loss=1.384, reward_mean=0.020, reward_bound=0.000, batch=10\n",
      "13: loss=1.382, reward_mean=0.010, reward_bound=0.000, batch=11\n",
      "14: loss=1.380, reward_mean=0.010, reward_bound=0.000, batch=12\n",
      "15: loss=1.381, reward_mean=0.010, reward_bound=0.000, batch=13\n",
      "16: loss=1.379, reward_mean=0.020, reward_bound=0.000, batch=15\n",
      "17: loss=1.380, reward_mean=0.010, reward_bound=0.000, batch=16\n",
      "18: loss=1.380, reward_mean=0.020, reward_bound=0.000, batch=18\n",
      "19: loss=1.380, reward_mean=0.010, reward_bound=0.000, batch=19\n",
      "20: loss=1.381, reward_mean=0.010, reward_bound=0.000, batch=20\n",
      "21: loss=1.381, reward_mean=0.020, reward_bound=0.000, batch=22\n",
      "22: loss=1.380, reward_mean=0.010, reward_bound=0.000, batch=23\n",
      "23: loss=1.381, reward_mean=0.020, reward_bound=0.000, batch=25\n",
      "24: loss=1.380, reward_mean=0.020, reward_bound=0.000, batch=27\n",
      "25: loss=1.378, reward_mean=0.030, reward_bound=0.000, batch=30\n",
      "26: loss=1.377, reward_mean=0.010, reward_bound=0.000, batch=31\n",
      "27: loss=1.375, reward_mean=0.010, reward_bound=0.000, batch=32\n",
      "28: loss=1.377, reward_mean=0.020, reward_bound=0.000, batch=34\n",
      "29: loss=1.376, reward_mean=0.000, reward_bound=0.000, batch=34\n",
      "30: loss=1.377, reward_mean=0.020, reward_bound=0.000, batch=36\n",
      "31: loss=1.376, reward_mean=0.010, reward_bound=0.000, batch=37\n",
      "32: loss=1.372, reward_mean=0.060, reward_bound=0.000, batch=43\n",
      "33: loss=1.373, reward_mean=0.010, reward_bound=0.000, batch=44\n",
      "34: loss=1.372, reward_mean=0.010, reward_bound=0.000, batch=45\n",
      "35: loss=1.370, reward_mean=0.030, reward_bound=0.000, batch=48\n",
      "36: loss=1.369, reward_mean=0.020, reward_bound=0.000, batch=50\n",
      "37: loss=1.370, reward_mean=0.010, reward_bound=0.000, batch=51\n",
      "38: loss=1.369, reward_mean=0.030, reward_bound=0.000, batch=54\n",
      "39: loss=1.369, reward_mean=0.000, reward_bound=0.000, batch=54\n",
      "40: loss=1.368, reward_mean=0.000, reward_bound=0.000, batch=54\n",
      "41: loss=1.370, reward_mean=0.030, reward_bound=0.000, batch=57\n",
      "42: loss=1.371, reward_mean=0.030, reward_bound=0.000, batch=60\n",
      "43: loss=1.370, reward_mean=0.010, reward_bound=0.000, batch=61\n",
      "44: loss=1.370, reward_mean=0.010, reward_bound=0.000, batch=62\n",
      "45: loss=1.369, reward_mean=0.010, reward_bound=0.000, batch=63\n",
      "46: loss=1.370, reward_mean=0.040, reward_bound=0.000, batch=67\n",
      "47: loss=1.370, reward_mean=0.010, reward_bound=0.000, batch=68\n",
      "48: loss=1.370, reward_mean=0.020, reward_bound=0.000, batch=70\n",
      "49: loss=1.369, reward_mean=0.010, reward_bound=0.000, batch=71\n",
      "50: loss=1.369, reward_mean=0.010, reward_bound=0.000, batch=72\n",
      "51: loss=1.368, reward_mean=0.000, reward_bound=0.000, batch=72\n",
      "52: loss=1.368, reward_mean=0.030, reward_bound=0.000, batch=75\n",
      "53: loss=1.368, reward_mean=0.010, reward_bound=0.000, batch=76\n",
      "54: loss=1.367, reward_mean=0.020, reward_bound=0.000, batch=78\n",
      "55: loss=1.367, reward_mean=0.010, reward_bound=0.000, batch=79\n",
      "56: loss=1.366, reward_mean=0.010, reward_bound=0.000, batch=80\n",
      "57: loss=1.364, reward_mean=0.020, reward_bound=0.000, batch=82\n",
      "58: loss=1.364, reward_mean=0.020, reward_bound=0.000, batch=84\n",
      "59: loss=1.364, reward_mean=0.000, reward_bound=0.000, batch=84\n",
      "60: loss=1.363, reward_mean=0.000, reward_bound=0.000, batch=84\n",
      "61: loss=1.363, reward_mean=0.020, reward_bound=0.000, batch=86\n",
      "62: loss=1.363, reward_mean=0.010, reward_bound=0.000, batch=87\n",
      "63: loss=1.362, reward_mean=0.010, reward_bound=0.000, batch=88\n",
      "64: loss=1.363, reward_mean=0.010, reward_bound=0.000, batch=89\n",
      "65: loss=1.362, reward_mean=0.020, reward_bound=0.000, batch=91\n",
      "66: loss=1.362, reward_mean=0.010, reward_bound=0.000, batch=92\n",
      "67: loss=1.362, reward_mean=0.010, reward_bound=0.000, batch=93\n",
      "68: loss=1.362, reward_mean=0.000, reward_bound=0.000, batch=93\n",
      "69: loss=1.362, reward_mean=0.020, reward_bound=0.000, batch=95\n",
      "70: loss=1.361, reward_mean=0.020, reward_bound=0.000, batch=97\n",
      "71: loss=1.360, reward_mean=0.010, reward_bound=0.000, batch=98\n",
      "72: loss=1.359, reward_mean=0.020, reward_bound=0.000, batch=100\n",
      "73: loss=1.360, reward_mean=0.020, reward_bound=0.000, batch=102\n",
      "74: loss=1.359, reward_mean=0.040, reward_bound=0.000, batch=106\n",
      "75: loss=1.359, reward_mean=0.010, reward_bound=0.000, batch=107\n",
      "76: loss=1.358, reward_mean=0.020, reward_bound=0.000, batch=109\n",
      "77: loss=1.358, reward_mean=0.020, reward_bound=0.000, batch=111\n",
      "78: loss=1.357, reward_mean=0.030, reward_bound=0.000, batch=114\n",
      "79: loss=1.357, reward_mean=0.020, reward_bound=0.000, batch=116\n",
      "80: loss=1.357, reward_mean=0.010, reward_bound=0.000, batch=117\n",
      "81: loss=1.357, reward_mean=0.020, reward_bound=0.000, batch=119\n",
      "82: loss=1.357, reward_mean=0.030, reward_bound=0.000, batch=122\n",
      "83: loss=1.357, reward_mean=0.030, reward_bound=0.000, batch=125\n",
      "84: loss=1.356, reward_mean=0.030, reward_bound=0.000, batch=128\n",
      "85: loss=1.356, reward_mean=0.020, reward_bound=0.000, batch=130\n",
      "86: loss=1.355, reward_mean=0.030, reward_bound=0.000, batch=133\n",
      "87: loss=1.355, reward_mean=0.010, reward_bound=0.000, batch=134\n",
      "88: loss=1.354, reward_mean=0.060, reward_bound=0.000, batch=140\n",
      "89: loss=1.354, reward_mean=0.010, reward_bound=0.000, batch=141\n",
      "90: loss=1.354, reward_mean=0.010, reward_bound=0.000, batch=142\n",
      "91: loss=1.355, reward_mean=0.030, reward_bound=0.000, batch=145\n",
      "92: loss=1.354, reward_mean=0.060, reward_bound=0.000, batch=151\n",
      "93: loss=1.353, reward_mean=0.020, reward_bound=0.000, batch=153\n",
      "94: loss=1.353, reward_mean=0.010, reward_bound=0.000, batch=154\n",
      "95: loss=1.353, reward_mean=0.080, reward_bound=0.000, batch=162\n",
      "96: loss=1.352, reward_mean=0.030, reward_bound=0.000, batch=165\n",
      "97: loss=1.352, reward_mean=0.010, reward_bound=0.000, batch=166\n",
      "98: loss=1.351, reward_mean=0.000, reward_bound=0.000, batch=166\n",
      "99: loss=1.352, reward_mean=0.030, reward_bound=0.000, batch=169\n",
      "100: loss=1.351, reward_mean=0.020, reward_bound=0.000, batch=171\n",
      "101: loss=1.351, reward_mean=0.010, reward_bound=0.000, batch=172\n",
      "102: loss=1.351, reward_mean=0.000, reward_bound=0.000, batch=172\n",
      "103: loss=1.351, reward_mean=0.010, reward_bound=0.000, batch=173\n",
      "104: loss=1.350, reward_mean=0.030, reward_bound=0.000, batch=176\n",
      "105: loss=1.350, reward_mean=0.010, reward_bound=0.000, batch=177\n",
      "106: loss=1.350, reward_mean=0.010, reward_bound=0.000, batch=178\n",
      "107: loss=1.349, reward_mean=0.050, reward_bound=0.000, batch=183\n",
      "108: loss=1.349, reward_mean=0.030, reward_bound=0.000, batch=186\n",
      "109: loss=1.349, reward_mean=0.030, reward_bound=0.000, batch=189\n",
      "110: loss=1.349, reward_mean=0.020, reward_bound=0.000, batch=191\n",
      "111: loss=1.349, reward_mean=0.030, reward_bound=0.000, batch=194\n",
      "112: loss=1.349, reward_mean=0.000, reward_bound=0.000, batch=194\n",
      "113: loss=1.348, reward_mean=0.030, reward_bound=0.000, batch=197\n",
      "114: loss=1.347, reward_mean=0.050, reward_bound=0.000, batch=202\n",
      "115: loss=1.347, reward_mean=0.010, reward_bound=0.000, batch=203\n",
      "116: loss=1.346, reward_mean=0.030, reward_bound=0.000, batch=206\n",
      "117: loss=1.345, reward_mean=0.030, reward_bound=0.000, batch=209\n",
      "118: loss=1.345, reward_mean=0.060, reward_bound=0.000, batch=215\n",
      "119: loss=1.344, reward_mean=0.020, reward_bound=0.000, batch=217\n",
      "120: loss=1.345, reward_mean=0.010, reward_bound=0.000, batch=218\n",
      "121: loss=1.344, reward_mean=0.020, reward_bound=0.000, batch=220\n",
      "122: loss=1.344, reward_mean=0.020, reward_bound=0.000, batch=222\n",
      "123: loss=1.344, reward_mean=0.030, reward_bound=0.004, batch=225\n",
      "124: loss=1.343, reward_mean=0.000, reward_bound=0.000, batch=225\n",
      "125: loss=1.343, reward_mean=0.020, reward_bound=0.003, batch=227\n",
      "126: loss=1.343, reward_mean=0.000, reward_bound=0.000, batch=227\n",
      "127: loss=1.340, reward_mean=0.050, reward_bound=0.037, batch=229\n",
      "128: loss=1.340, reward_mean=0.020, reward_bound=0.042, batch=230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129: loss=1.338, reward_mean=0.030, reward_bound=0.068, batch=231\n",
      "130: loss=1.336, reward_mean=0.040, reward_bound=0.072, batch=231\n",
      "131: loss=1.336, reward_mean=0.000, reward_bound=0.000, batch=231\n",
      "132: loss=1.336, reward_mean=0.010, reward_bound=0.080, batch=230\n",
      "133: loss=1.336, reward_mean=0.050, reward_bound=0.096, batch=231\n",
      "134: loss=1.334, reward_mean=0.020, reward_bound=0.098, batch=226\n",
      "135: loss=1.333, reward_mean=0.010, reward_bound=0.000, batch=227\n",
      "136: loss=1.332, reward_mean=0.030, reward_bound=0.105, batch=229\n",
      "137: loss=1.330, reward_mean=0.050, reward_bound=0.109, batch=225\n",
      "138: loss=1.330, reward_mean=0.020, reward_bound=0.024, batch=227\n",
      "139: loss=1.330, reward_mean=0.020, reward_bound=0.064, batch=229\n",
      "140: loss=1.329, reward_mean=0.020, reward_bound=0.096, batch=230\n",
      "141: loss=1.329, reward_mean=0.020, reward_bound=0.122, batch=222\n",
      "142: loss=1.328, reward_mean=0.030, reward_bound=0.041, batch=225\n",
      "143: loss=1.328, reward_mean=0.020, reward_bound=0.027, batch=227\n",
      "144: loss=1.325, reward_mean=0.040, reward_bound=0.135, batch=222\n",
      "145: loss=1.325, reward_mean=0.020, reward_bound=0.000, batch=224\n",
      "146: loss=1.325, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "147: loss=1.324, reward_mean=0.010, reward_bound=0.000, batch=227\n",
      "148: loss=1.323, reward_mean=0.020, reward_bound=0.047, batch=229\n",
      "149: loss=1.322, reward_mean=0.020, reward_bound=0.095, batch=230\n",
      "150: loss=1.318, reward_mean=0.020, reward_bound=0.150, batch=222\n",
      "151: loss=1.313, reward_mean=0.050, reward_bound=0.167, batch=218\n",
      "152: loss=1.312, reward_mean=0.010, reward_bound=0.000, batch=219\n",
      "153: loss=1.311, reward_mean=0.020, reward_bound=0.000, batch=221\n",
      "154: loss=1.312, reward_mean=0.030, reward_bound=0.000, batch=224\n",
      "155: loss=1.311, reward_mean=0.030, reward_bound=0.109, batch=227\n",
      "156: loss=1.309, reward_mean=0.050, reward_bound=0.185, batch=218\n",
      "157: loss=1.304, reward_mean=0.060, reward_bound=0.206, batch=199\n",
      "158: loss=1.304, reward_mean=0.010, reward_bound=0.000, batch=200\n",
      "159: loss=1.301, reward_mean=0.040, reward_bound=0.000, batch=204\n",
      "160: loss=1.300, reward_mean=0.040, reward_bound=0.000, batch=208\n",
      "161: loss=1.299, reward_mean=0.010, reward_bound=0.000, batch=209\n",
      "162: loss=1.298, reward_mean=0.030, reward_bound=0.000, batch=212\n",
      "163: loss=1.298, reward_mean=0.020, reward_bound=0.000, batch=214\n",
      "164: loss=1.296, reward_mean=0.040, reward_bound=0.000, batch=218\n",
      "165: loss=1.296, reward_mean=0.070, reward_bound=0.115, batch=222\n",
      "166: loss=1.296, reward_mean=0.060, reward_bound=0.167, batch=224\n",
      "167: loss=1.295, reward_mean=0.050, reward_bound=0.185, batch=225\n",
      "168: loss=1.295, reward_mean=0.000, reward_bound=0.000, batch=225\n",
      "169: loss=1.294, reward_mean=0.050, reward_bound=0.229, batch=209\n",
      "170: loss=1.294, reward_mean=0.030, reward_bound=0.000, batch=212\n",
      "171: loss=1.294, reward_mean=0.030, reward_bound=0.000, batch=215\n",
      "172: loss=1.282, reward_mean=0.090, reward_bound=0.254, batch=195\n",
      "173: loss=1.281, reward_mean=0.030, reward_bound=0.000, batch=198\n",
      "174: loss=1.282, reward_mean=0.040, reward_bound=0.000, batch=202\n",
      "175: loss=1.281, reward_mean=0.030, reward_bound=0.000, batch=205\n",
      "176: loss=1.282, reward_mean=0.030, reward_bound=0.000, batch=208\n",
      "177: loss=1.282, reward_mean=0.020, reward_bound=0.000, batch=210\n",
      "178: loss=1.281, reward_mean=0.040, reward_bound=0.000, batch=214\n",
      "179: loss=1.279, reward_mean=0.040, reward_bound=0.000, batch=218\n",
      "180: loss=1.278, reward_mean=0.040, reward_bound=0.001, batch=222\n",
      "181: loss=1.278, reward_mean=0.020, reward_bound=0.000, batch=224\n",
      "182: loss=1.273, reward_mean=0.050, reward_bound=0.086, batch=227\n",
      "183: loss=1.272, reward_mean=0.000, reward_bound=0.000, batch=227\n",
      "184: loss=1.274, reward_mean=0.060, reward_bound=0.119, batch=229\n",
      "185: loss=1.271, reward_mean=0.040, reward_bound=0.150, batch=229\n",
      "186: loss=1.270, reward_mean=0.020, reward_bound=0.174, batch=230\n",
      "187: loss=1.268, reward_mean=0.030, reward_bound=0.200, batch=231\n",
      "188: loss=1.269, reward_mean=0.050, reward_bound=0.229, batch=230\n",
      "189: loss=1.268, reward_mean=0.060, reward_bound=0.254, batch=229\n",
      "190: loss=1.264, reward_mean=0.060, reward_bound=0.282, batch=198\n",
      "191: loss=1.263, reward_mean=0.030, reward_bound=0.000, batch=201\n",
      "192: loss=1.262, reward_mean=0.020, reward_bound=0.000, batch=203\n",
      "193: loss=1.262, reward_mean=0.060, reward_bound=0.000, batch=209\n",
      "194: loss=1.260, reward_mean=0.090, reward_bound=0.092, batch=216\n",
      "195: loss=1.262, reward_mean=0.050, reward_bound=0.036, batch=221\n",
      "196: loss=1.257, reward_mean=0.060, reward_bound=0.122, batch=224\n",
      "197: loss=1.253, reward_mean=0.090, reward_bound=0.200, batch=227\n",
      "198: loss=1.252, reward_mean=0.040, reward_bound=0.202, batch=229\n",
      "199: loss=1.252, reward_mean=0.070, reward_bound=0.229, batch=229\n",
      "200: loss=1.250, reward_mean=0.040, reward_bound=0.254, batch=228\n",
      "201: loss=1.251, reward_mean=0.040, reward_bound=0.282, batch=224\n",
      "202: loss=1.250, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "203: loss=1.249, reward_mean=0.070, reward_bound=0.241, batch=228\n",
      "204: loss=1.248, reward_mean=0.030, reward_bound=0.161, batch=229\n",
      "205: loss=1.237, reward_mean=0.060, reward_bound=0.314, batch=194\n",
      "206: loss=1.234, reward_mean=0.080, reward_bound=0.000, batch=202\n",
      "207: loss=1.232, reward_mean=0.020, reward_bound=0.000, batch=204\n",
      "208: loss=1.233, reward_mean=0.040, reward_bound=0.000, batch=208\n",
      "209: loss=1.235, reward_mean=0.050, reward_bound=0.000, batch=213\n",
      "210: loss=1.234, reward_mean=0.020, reward_bound=0.000, batch=215\n",
      "211: loss=1.232, reward_mean=0.050, reward_bound=0.008, batch=220\n",
      "212: loss=1.230, reward_mean=0.040, reward_bound=0.018, batch=224\n",
      "213: loss=1.231, reward_mean=0.040, reward_bound=0.041, batch=227\n",
      "214: loss=1.229, reward_mean=0.020, reward_bound=0.034, batch=229\n",
      "215: loss=1.227, reward_mean=0.040, reward_bound=0.093, batch=230\n",
      "216: loss=1.225, reward_mean=0.070, reward_bound=0.150, batch=230\n",
      "217: loss=1.225, reward_mean=0.020, reward_bound=0.167, batch=228\n",
      "218: loss=1.227, reward_mean=0.020, reward_bound=0.206, batch=226\n",
      "219: loss=1.224, reward_mean=0.040, reward_bound=0.229, batch=227\n",
      "220: loss=1.224, reward_mean=0.040, reward_bound=0.202, batch=229\n",
      "221: loss=1.222, reward_mean=0.070, reward_bound=0.254, batch=229\n",
      "222: loss=1.221, reward_mean=0.050, reward_bound=0.237, batch=230\n",
      "223: loss=1.222, reward_mean=0.040, reward_bound=0.282, batch=222\n",
      "224: loss=1.221, reward_mean=0.050, reward_bound=0.245, batch=225\n",
      "225: loss=1.220, reward_mean=0.070, reward_bound=0.289, batch=227\n",
      "226: loss=1.219, reward_mean=0.050, reward_bound=0.308, batch=229\n",
      "227: loss=1.222, reward_mean=0.060, reward_bound=0.314, batch=228\n",
      "228: loss=1.223, reward_mean=0.020, reward_bound=0.144, batch=229\n",
      "229: loss=1.216, reward_mean=0.040, reward_bound=0.349, batch=178\n",
      "230: loss=1.215, reward_mean=0.100, reward_bound=0.000, batch=188\n",
      "231: loss=1.212, reward_mean=0.040, reward_bound=0.000, batch=192\n",
      "232: loss=1.208, reward_mean=0.080, reward_bound=0.000, batch=200\n",
      "233: loss=1.208, reward_mean=0.030, reward_bound=0.000, batch=203\n",
      "234: loss=1.209, reward_mean=0.070, reward_bound=0.000, batch=210\n",
      "235: loss=1.210, reward_mean=0.080, reward_bound=0.052, batch=217\n",
      "236: loss=1.209, reward_mean=0.060, reward_bound=0.065, batch=221\n",
      "237: loss=1.207, reward_mean=0.060, reward_bound=0.098, batch=224\n",
      "238: loss=1.206, reward_mean=0.070, reward_bound=0.133, batch=227\n",
      "239: loss=1.200, reward_mean=0.050, reward_bound=0.147, batch=229\n",
      "240: loss=1.198, reward_mean=0.040, reward_bound=0.150, batch=228\n",
      "241: loss=1.198, reward_mean=0.030, reward_bound=0.185, batch=227\n",
      "242: loss=1.196, reward_mean=0.070, reward_bound=0.206, batch=226\n",
      "243: loss=1.194, reward_mean=0.090, reward_bound=0.241, batch=228\n",
      "244: loss=1.196, reward_mean=0.070, reward_bound=0.254, batch=228\n",
      "245: loss=1.196, reward_mean=0.040, reward_bound=0.282, batch=226\n",
      "246: loss=1.197, reward_mean=0.050, reward_bound=0.314, batch=222\n",
      "247: loss=1.198, reward_mean=0.040, reward_bound=0.349, batch=218\n",
      "248: loss=1.196, reward_mean=0.070, reward_bound=0.353, batch=222\n",
      "249: loss=1.198, reward_mean=0.050, reward_bound=0.254, batch=224\n",
      "250: loss=1.196, reward_mean=0.030, reward_bound=0.229, batch=227\n",
      "251: loss=1.196, reward_mean=0.100, reward_bound=0.302, batch=229\n",
      "252: loss=1.195, reward_mean=0.020, reward_bound=0.295, batch=230\n",
      "253: loss=1.195, reward_mean=0.050, reward_bound=0.296, batch=231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254: loss=1.195, reward_mean=0.020, reward_bound=0.167, batch=231\n",
      "255: loss=1.194, reward_mean=0.040, reward_bound=0.314, batch=231\n",
      "256: loss=1.194, reward_mean=0.040, reward_bound=0.349, batch=230\n",
      "257: loss=1.175, reward_mean=0.070, reward_bound=0.387, batch=164\n",
      "258: loss=1.172, reward_mean=0.110, reward_bound=0.000, batch=175\n",
      "259: loss=1.169, reward_mean=0.040, reward_bound=0.000, batch=179\n",
      "260: loss=1.167, reward_mean=0.040, reward_bound=0.000, batch=183\n",
      "261: loss=1.170, reward_mean=0.080, reward_bound=0.000, batch=191\n",
      "262: loss=1.169, reward_mean=0.050, reward_bound=0.000, batch=196\n",
      "263: loss=1.168, reward_mean=0.040, reward_bound=0.000, batch=200\n",
      "264: loss=1.169, reward_mean=0.080, reward_bound=0.000, batch=208\n",
      "265: loss=1.166, reward_mean=0.070, reward_bound=0.007, batch=215\n",
      "266: loss=1.165, reward_mean=0.080, reward_bound=0.091, batch=220\n",
      "267: loss=1.165, reward_mean=0.040, reward_bound=0.069, batch=224\n",
      "268: loss=1.166, reward_mean=0.040, reward_bound=0.096, batch=227\n",
      "269: loss=1.162, reward_mean=0.040, reward_bound=0.109, batch=230\n",
      "270: loss=1.167, reward_mean=0.020, reward_bound=0.109, batch=228\n",
      "271: loss=1.165, reward_mean=0.040, reward_bound=0.124, batch=229\n",
      "272: loss=1.165, reward_mean=0.040, reward_bound=0.150, batch=228\n",
      "273: loss=1.167, reward_mean=0.020, reward_bound=0.167, batch=226\n",
      "274: loss=1.165, reward_mean=0.030, reward_bound=0.160, batch=228\n",
      "275: loss=1.165, reward_mean=0.070, reward_bound=0.185, batch=227\n",
      "276: loss=1.165, reward_mean=0.040, reward_bound=0.158, batch=229\n",
      "277: loss=1.164, reward_mean=0.040, reward_bound=0.206, batch=229\n",
      "278: loss=1.160, reward_mean=0.050, reward_bound=0.229, batch=229\n",
      "279: loss=1.160, reward_mean=0.060, reward_bound=0.254, batch=226\n",
      "280: loss=1.161, reward_mean=0.060, reward_bound=0.282, batch=226\n",
      "281: loss=1.162, reward_mean=0.080, reward_bound=0.314, batch=218\n",
      "282: loss=1.164, reward_mean=0.040, reward_bound=0.015, batch=222\n",
      "283: loss=1.161, reward_mean=0.050, reward_bound=0.161, batch=225\n",
      "284: loss=1.161, reward_mean=0.070, reward_bound=0.229, batch=225\n",
      "285: loss=1.160, reward_mean=0.040, reward_bound=0.199, batch=227\n",
      "286: loss=1.157, reward_mean=0.050, reward_bound=0.254, batch=228\n",
      "287: loss=1.158, reward_mean=0.050, reward_bound=0.282, batch=228\n",
      "288: loss=1.157, reward_mean=0.040, reward_bound=0.260, batch=229\n",
      "289: loss=1.157, reward_mean=0.060, reward_bound=0.314, batch=229\n",
      "290: loss=1.157, reward_mean=0.050, reward_bound=0.292, batch=230\n",
      "291: loss=1.156, reward_mean=0.030, reward_bound=0.306, batch=231\n",
      "292: loss=1.154, reward_mean=0.060, reward_bound=0.349, batch=218\n",
      "293: loss=1.154, reward_mean=0.020, reward_bound=0.000, batch=220\n",
      "294: loss=1.152, reward_mean=0.070, reward_bound=0.282, batch=222\n",
      "295: loss=1.153, reward_mean=0.090, reward_bound=0.314, batch=224\n",
      "296: loss=1.153, reward_mean=0.070, reward_bound=0.387, batch=211\n",
      "297: loss=1.151, reward_mean=0.050, reward_bound=0.000, batch=216\n",
      "298: loss=1.148, reward_mean=0.100, reward_bound=0.230, batch=221\n",
      "299: loss=1.152, reward_mean=0.100, reward_bound=0.282, batch=222\n",
      "300: loss=1.150, reward_mean=0.060, reward_bound=0.314, batch=224\n",
      "301: loss=1.149, reward_mean=0.050, reward_bound=0.182, batch=227\n",
      "302: loss=1.148, reward_mean=0.080, reward_bound=0.277, batch=229\n",
      "303: loss=1.148, reward_mean=0.050, reward_bound=0.295, batch=230\n",
      "304: loss=1.148, reward_mean=0.110, reward_bound=0.338, batch=231\n",
      "305: loss=1.148, reward_mean=0.040, reward_bound=0.349, batch=225\n",
      "306: loss=1.147, reward_mean=0.060, reward_bound=0.329, batch=227\n",
      "307: loss=1.148, reward_mean=0.040, reward_bound=0.224, batch=229\n",
      "308: loss=1.147, reward_mean=0.060, reward_bound=0.324, batch=230\n",
      "309: loss=1.146, reward_mean=0.050, reward_bound=0.356, batch=231\n",
      "310: loss=1.149, reward_mean=0.050, reward_bound=0.387, batch=224\n",
      "311: loss=1.147, reward_mean=0.050, reward_bound=0.359, batch=227\n",
      "312: loss=1.147, reward_mean=0.120, reward_bound=0.387, batch=228\n",
      "313: loss=1.139, reward_mean=0.090, reward_bound=0.430, batch=120\n",
      "314: loss=1.148, reward_mean=0.070, reward_bound=0.000, batch=127\n",
      "315: loss=1.148, reward_mean=0.050, reward_bound=0.000, batch=132\n",
      "316: loss=1.144, reward_mean=0.040, reward_bound=0.000, batch=136\n",
      "317: loss=1.131, reward_mean=0.090, reward_bound=0.000, batch=145\n",
      "318: loss=1.120, reward_mean=0.090, reward_bound=0.000, batch=154\n",
      "319: loss=1.120, reward_mean=0.030, reward_bound=0.000, batch=157\n",
      "320: loss=1.122, reward_mean=0.070, reward_bound=0.000, batch=164\n",
      "321: loss=1.120, reward_mean=0.030, reward_bound=0.000, batch=167\n",
      "322: loss=1.116, reward_mean=0.070, reward_bound=0.000, batch=174\n",
      "323: loss=1.111, reward_mean=0.090, reward_bound=0.000, batch=183\n",
      "324: loss=1.106, reward_mean=0.050, reward_bound=0.000, batch=188\n",
      "325: loss=1.104, reward_mean=0.100, reward_bound=0.000, batch=198\n",
      "326: loss=1.102, reward_mean=0.080, reward_bound=0.000, batch=206\n",
      "327: loss=1.099, reward_mean=0.060, reward_bound=0.000, batch=212\n",
      "328: loss=1.095, reward_mean=0.060, reward_bound=0.008, batch=218\n",
      "329: loss=1.096, reward_mean=0.040, reward_bound=0.003, batch=222\n",
      "330: loss=1.094, reward_mean=0.060, reward_bound=0.044, batch=225\n",
      "331: loss=1.091, reward_mean=0.070, reward_bound=0.098, batch=226\n",
      "332: loss=1.093, reward_mean=0.060, reward_bound=0.122, batch=227\n",
      "333: loss=1.092, reward_mean=0.110, reward_bound=0.163, batch=229\n",
      "334: loss=1.088, reward_mean=0.040, reward_bound=0.167, batch=224\n",
      "335: loss=1.086, reward_mean=0.050, reward_bound=0.185, batch=218\n",
      "336: loss=1.091, reward_mean=0.090, reward_bound=0.206, batch=220\n",
      "337: loss=1.090, reward_mean=0.050, reward_bound=0.194, batch=224\n",
      "338: loss=1.093, reward_mean=0.080, reward_bound=0.229, batch=223\n",
      "339: loss=1.093, reward_mean=0.050, reward_bound=0.254, batch=216\n",
      "340: loss=1.086, reward_mean=0.080, reward_bound=0.196, batch=221\n",
      "341: loss=1.085, reward_mean=0.040, reward_bound=0.122, batch=224\n",
      "342: loss=1.083, reward_mean=0.040, reward_bound=0.143, batch=227\n",
      "343: loss=1.083, reward_mean=0.050, reward_bound=0.195, batch=229\n",
      "344: loss=1.086, reward_mean=0.080, reward_bound=0.254, batch=229\n",
      "345: loss=1.085, reward_mean=0.070, reward_bound=0.265, batch=230\n",
      "346: loss=1.092, reward_mean=0.050, reward_bound=0.282, batch=213\n",
      "347: loss=1.091, reward_mean=0.100, reward_bound=0.290, batch=219\n",
      "348: loss=1.089, reward_mean=0.120, reward_bound=0.295, batch=223\n",
      "349: loss=1.090, reward_mean=0.070, reward_bound=0.244, batch=226\n",
      "350: loss=1.089, reward_mean=0.120, reward_bound=0.314, batch=208\n",
      "351: loss=1.090, reward_mean=0.050, reward_bound=0.000, batch=213\n",
      "352: loss=1.087, reward_mean=0.040, reward_bound=0.000, batch=217\n",
      "353: loss=1.084, reward_mean=0.080, reward_bound=0.210, batch=222\n",
      "354: loss=1.080, reward_mean=0.040, reward_bound=0.088, batch=225\n",
      "355: loss=1.083, reward_mean=0.060, reward_bound=0.260, batch=227\n",
      "356: loss=1.081, reward_mean=0.030, reward_bound=0.210, batch=229\n",
      "357: loss=1.081, reward_mean=0.060, reward_bound=0.250, batch=230\n",
      "358: loss=1.083, reward_mean=0.050, reward_bound=0.282, batch=229\n",
      "359: loss=1.082, reward_mean=0.090, reward_bound=0.314, batch=228\n",
      "360: loss=1.096, reward_mean=0.080, reward_bound=0.349, batch=208\n",
      "361: loss=1.096, reward_mean=0.050, reward_bound=0.000, batch=213\n",
      "362: loss=1.092, reward_mean=0.060, reward_bound=0.137, batch=219\n",
      "363: loss=1.089, reward_mean=0.040, reward_bound=0.054, batch=223\n",
      "364: loss=1.086, reward_mean=0.030, reward_bound=0.081, batch=226\n",
      "365: loss=1.087, reward_mean=0.050, reward_bound=0.143, batch=228\n",
      "366: loss=1.089, reward_mean=0.070, reward_bound=0.167, batch=228\n",
      "367: loss=1.092, reward_mean=0.100, reward_bound=0.257, batch=229\n",
      "368: loss=1.091, reward_mean=0.050, reward_bound=0.295, batch=230\n",
      "369: loss=1.090, reward_mean=0.100, reward_bound=0.338, batch=231\n",
      "370: loss=1.090, reward_mean=0.070, reward_bound=0.349, batch=229\n",
      "371: loss=1.090, reward_mean=0.040, reward_bound=0.328, batch=230\n",
      "372: loss=1.088, reward_mean=0.070, reward_bound=0.376, batch=231\n",
      "373: loss=1.095, reward_mean=0.080, reward_bound=0.387, batch=194\n",
      "374: loss=1.094, reward_mean=0.090, reward_bound=0.000, batch=203\n",
      "375: loss=1.094, reward_mean=0.040, reward_bound=0.000, batch=207\n",
      "376: loss=1.093, reward_mean=0.060, reward_bound=0.000, batch=213\n",
      "377: loss=1.096, reward_mean=0.070, reward_bound=0.037, batch=219\n",
      "378: loss=1.088, reward_mean=0.080, reward_bound=0.098, batch=222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379: loss=1.092, reward_mean=0.070, reward_bound=0.145, batch=225\n",
      "380: loss=1.090, reward_mean=0.030, reward_bound=0.167, batch=226\n",
      "381: loss=1.082, reward_mean=0.070, reward_bound=0.229, batch=225\n",
      "382: loss=1.086, reward_mean=0.090, reward_bound=0.254, batch=220\n",
      "383: loss=1.088, reward_mean=0.040, reward_bound=0.130, batch=224\n",
      "384: loss=1.084, reward_mean=0.040, reward_bound=0.185, batch=226\n",
      "385: loss=1.084, reward_mean=0.040, reward_bound=0.207, batch=228\n",
      "386: loss=1.080, reward_mean=0.090, reward_bound=0.282, batch=226\n",
      "387: loss=1.080, reward_mean=0.030, reward_bound=0.164, batch=228\n",
      "388: loss=1.085, reward_mean=0.070, reward_bound=0.314, batch=221\n",
      "389: loss=1.082, reward_mean=0.070, reward_bound=0.314, batch=224\n",
      "390: loss=1.081, reward_mean=0.060, reward_bound=0.308, batch=227\n",
      "391: loss=1.081, reward_mean=0.040, reward_bound=0.342, batch=229\n",
      "392: loss=1.082, reward_mean=0.070, reward_bound=0.349, batch=226\n",
      "393: loss=1.081, reward_mean=0.060, reward_bound=0.241, batch=228\n",
      "394: loss=1.081, reward_mean=0.090, reward_bound=0.260, batch=229\n",
      "395: loss=1.083, reward_mean=0.090, reward_bound=0.364, batch=230\n",
      "396: loss=1.082, reward_mean=0.070, reward_bound=0.376, batch=231\n",
      "397: loss=1.085, reward_mean=0.050, reward_bound=0.387, batch=218\n",
      "398: loss=1.080, reward_mean=0.070, reward_bound=0.211, batch=222\n",
      "399: loss=1.081, reward_mean=0.070, reward_bound=0.254, batch=224\n",
      "400: loss=1.079, reward_mean=0.060, reward_bound=0.185, batch=226\n",
      "401: loss=1.083, reward_mean=0.100, reward_bound=0.314, batch=227\n",
      "402: loss=1.083, reward_mean=0.090, reward_bound=0.380, batch=229\n",
      "403: loss=1.085, reward_mean=0.120, reward_bound=0.387, batch=229\n",
      "404: loss=1.085, reward_mean=0.040, reward_bound=0.229, batch=229\n",
      "405: loss=1.107, reward_mean=0.080, reward_bound=0.430, batch=178\n",
      "406: loss=1.104, reward_mean=0.060, reward_bound=0.000, batch=184\n",
      "407: loss=1.100, reward_mean=0.070, reward_bound=0.000, batch=191\n",
      "408: loss=1.103, reward_mean=0.050, reward_bound=0.000, batch=196\n",
      "409: loss=1.104, reward_mean=0.020, reward_bound=0.000, batch=198\n",
      "410: loss=1.102, reward_mean=0.070, reward_bound=0.000, batch=205\n",
      "411: loss=1.106, reward_mean=0.100, reward_bound=0.066, batch=213\n",
      "412: loss=1.106, reward_mean=0.040, reward_bound=0.000, batch=217\n",
      "413: loss=1.103, reward_mean=0.050, reward_bound=0.057, batch=222\n",
      "414: loss=1.104, reward_mean=0.030, reward_bound=0.022, batch=225\n",
      "415: loss=1.104, reward_mean=0.040, reward_bound=0.073, batch=227\n",
      "416: loss=1.098, reward_mean=0.100, reward_bound=0.163, batch=229\n",
      "417: loss=1.094, reward_mean=0.070, reward_bound=0.185, batch=227\n",
      "418: loss=1.097, reward_mean=0.070, reward_bound=0.206, batch=226\n",
      "419: loss=1.094, reward_mean=0.100, reward_bound=0.229, batch=226\n",
      "420: loss=1.093, reward_mean=0.050, reward_bound=0.241, batch=228\n",
      "421: loss=1.095, reward_mean=0.070, reward_bound=0.254, batch=226\n",
      "422: loss=1.094, reward_mean=0.040, reward_bound=0.268, batch=228\n",
      "423: loss=1.085, reward_mean=0.060, reward_bound=0.282, batch=223\n",
      "424: loss=1.085, reward_mean=0.050, reward_bound=0.314, batch=218\n",
      "425: loss=1.081, reward_mean=0.030, reward_bound=0.000, batch=221\n",
      "426: loss=1.084, reward_mean=0.090, reward_bound=0.282, batch=224\n",
      "427: loss=1.090, reward_mean=0.080, reward_bound=0.349, batch=214\n",
      "428: loss=1.094, reward_mean=0.100, reward_bound=0.387, batch=211\n",
      "429: loss=1.090, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "430: loss=1.088, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "431: loss=1.093, reward_mean=0.060, reward_bound=0.229, batch=224\n",
      "432: loss=1.092, reward_mean=0.050, reward_bound=0.252, batch=227\n",
      "433: loss=1.088, reward_mean=0.080, reward_bound=0.277, batch=229\n",
      "434: loss=1.088, reward_mean=0.090, reward_bound=0.250, batch=230\n",
      "435: loss=1.092, reward_mean=0.070, reward_bound=0.314, batch=228\n",
      "436: loss=1.093, reward_mean=0.070, reward_bound=0.241, batch=229\n",
      "437: loss=1.085, reward_mean=0.040, reward_bound=0.349, batch=227\n",
      "438: loss=1.084, reward_mean=0.030, reward_bound=0.282, batch=228\n",
      "439: loss=1.082, reward_mean=0.060, reward_bound=0.321, batch=229\n",
      "440: loss=1.085, reward_mean=0.070, reward_bound=0.387, batch=226\n",
      "441: loss=1.084, reward_mean=0.040, reward_bound=0.250, batch=228\n",
      "442: loss=1.083, reward_mean=0.100, reward_bound=0.314, batch=228\n",
      "443: loss=1.084, reward_mean=0.070, reward_bound=0.392, batch=229\n",
      "444: loss=1.084, reward_mean=0.020, reward_bound=0.314, batch=229\n",
      "445: loss=1.084, reward_mean=0.020, reward_bound=0.155, batch=230\n",
      "446: loss=1.083, reward_mean=0.050, reward_bound=0.378, batch=231\n",
      "447: loss=1.096, reward_mean=0.060, reward_bound=0.430, batch=210\n",
      "448: loss=1.093, reward_mean=0.020, reward_bound=0.000, batch=212\n",
      "449: loss=1.090, reward_mean=0.080, reward_bound=0.101, batch=218\n",
      "450: loss=1.087, reward_mean=0.070, reward_bound=0.152, batch=222\n",
      "451: loss=1.084, reward_mean=0.070, reward_bound=0.172, batch=225\n",
      "452: loss=1.085, reward_mean=0.040, reward_bound=0.185, batch=226\n",
      "453: loss=1.087, reward_mean=0.090, reward_bound=0.229, batch=227\n",
      "454: loss=1.087, reward_mean=0.040, reward_bound=0.223, batch=229\n",
      "455: loss=1.090, reward_mean=0.080, reward_bound=0.265, batch=230\n",
      "456: loss=1.093, reward_mean=0.060, reward_bound=0.314, batch=226\n",
      "457: loss=1.093, reward_mean=0.070, reward_bound=0.250, batch=228\n",
      "458: loss=1.094, reward_mean=0.100, reward_bound=0.353, batch=229\n",
      "459: loss=1.094, reward_mean=0.060, reward_bound=0.387, batch=227\n",
      "460: loss=1.092, reward_mean=0.130, reward_bound=0.414, batch=229\n",
      "461: loss=1.093, reward_mean=0.060, reward_bound=0.342, batch=230\n",
      "462: loss=1.095, reward_mean=0.040, reward_bound=0.430, batch=227\n",
      "463: loss=1.093, reward_mean=0.060, reward_bound=0.430, batch=228\n",
      "464: loss=1.093, reward_mean=0.090, reward_bound=0.478, batch=230\n",
      "465: loss=1.094, reward_mean=0.120, reward_bound=0.464, batch=231\n",
      "466: loss=1.069, reward_mean=0.060, reward_bound=0.478, batch=88\n",
      "467: loss=1.061, reward_mean=0.070, reward_bound=0.000, batch=95\n",
      "468: loss=1.063, reward_mean=0.050, reward_bound=0.000, batch=100\n",
      "469: loss=1.062, reward_mean=0.060, reward_bound=0.000, batch=106\n",
      "470: loss=1.059, reward_mean=0.050, reward_bound=0.000, batch=111\n",
      "471: loss=1.058, reward_mean=0.070, reward_bound=0.000, batch=118\n",
      "472: loss=1.058, reward_mean=0.050, reward_bound=0.000, batch=123\n",
      "473: loss=1.055, reward_mean=0.080, reward_bound=0.000, batch=131\n",
      "474: loss=1.052, reward_mean=0.080, reward_bound=0.000, batch=139\n",
      "475: loss=1.049, reward_mean=0.080, reward_bound=0.000, batch=147\n",
      "476: loss=1.047, reward_mean=0.060, reward_bound=0.000, batch=153\n",
      "477: loss=1.041, reward_mean=0.060, reward_bound=0.000, batch=159\n",
      "478: loss=1.038, reward_mean=0.070, reward_bound=0.000, batch=166\n",
      "479: loss=1.029, reward_mean=0.110, reward_bound=0.000, batch=177\n",
      "480: loss=1.028, reward_mean=0.100, reward_bound=0.000, batch=187\n",
      "481: loss=1.024, reward_mean=0.030, reward_bound=0.000, batch=190\n",
      "482: loss=1.018, reward_mean=0.080, reward_bound=0.000, batch=198\n",
      "483: loss=1.017, reward_mean=0.080, reward_bound=0.000, batch=206\n",
      "484: loss=1.015, reward_mean=0.080, reward_bound=0.013, batch=214\n",
      "485: loss=1.009, reward_mean=0.110, reward_bound=0.064, batch=220\n",
      "486: loss=1.007, reward_mean=0.120, reward_bound=0.098, batch=223\n",
      "487: loss=1.006, reward_mean=0.070, reward_bound=0.109, batch=224\n",
      "488: loss=1.013, reward_mean=0.070, reward_bound=0.135, batch=221\n",
      "489: loss=1.018, reward_mean=0.060, reward_bound=0.150, batch=222\n",
      "490: loss=1.017, reward_mean=0.040, reward_bound=0.101, batch=225\n",
      "491: loss=1.011, reward_mean=0.080, reward_bound=0.167, batch=225\n",
      "492: loss=1.011, reward_mean=0.050, reward_bound=0.185, batch=222\n",
      "493: loss=1.008, reward_mean=0.070, reward_bound=0.206, batch=228\n",
      "494: loss=1.010, reward_mean=0.050, reward_bound=0.206, batch=220\n",
      "495: loss=1.007, reward_mean=0.110, reward_bound=0.229, batch=219\n",
      "496: loss=1.011, reward_mean=0.090, reward_bound=0.254, batch=209\n",
      "497: loss=1.009, reward_mean=0.100, reward_bound=0.239, batch=216\n",
      "498: loss=1.007, reward_mean=0.070, reward_bound=0.241, batch=221\n",
      "499: loss=1.008, reward_mean=0.090, reward_bound=0.254, batch=223\n",
      "500: loss=1.018, reward_mean=0.090, reward_bound=0.282, batch=208\n",
      "501: loss=1.016, reward_mean=0.100, reward_bound=0.140, batch=215\n",
      "502: loss=1.010, reward_mean=0.070, reward_bound=0.210, batch=220\n",
      "503: loss=1.009, reward_mean=0.090, reward_bound=0.229, batch=221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504: loss=1.010, reward_mean=0.080, reward_bound=0.282, batch=224\n",
      "505: loss=1.020, reward_mean=0.080, reward_bound=0.314, batch=198\n",
      "506: loss=1.013, reward_mean=0.110, reward_bound=0.098, batch=207\n",
      "507: loss=1.005, reward_mean=0.070, reward_bound=0.000, batch=214\n",
      "508: loss=1.005, reward_mean=0.060, reward_bound=0.020, batch=220\n",
      "509: loss=1.000, reward_mean=0.120, reward_bound=0.162, batch=224\n",
      "510: loss=1.004, reward_mean=0.050, reward_bound=0.167, batch=224\n",
      "511: loss=1.004, reward_mean=0.140, reward_bound=0.206, batch=226\n",
      "512: loss=1.005, reward_mean=0.090, reward_bound=0.284, batch=228\n",
      "513: loss=1.013, reward_mean=0.070, reward_bound=0.349, batch=194\n",
      "514: loss=1.009, reward_mean=0.060, reward_bound=0.000, batch=200\n",
      "515: loss=1.002, reward_mean=0.070, reward_bound=0.000, batch=207\n",
      "516: loss=0.995, reward_mean=0.050, reward_bound=0.000, batch=212\n",
      "517: loss=0.995, reward_mean=0.050, reward_bound=0.000, batch=217\n",
      "518: loss=0.994, reward_mean=0.040, reward_bound=0.000, batch=221\n",
      "519: loss=1.001, reward_mean=0.080, reward_bound=0.135, batch=224\n",
      "520: loss=0.999, reward_mean=0.050, reward_bound=0.165, batch=227\n",
      "521: loss=1.003, reward_mean=0.080, reward_bound=0.185, batch=226\n",
      "522: loss=0.997, reward_mean=0.080, reward_bound=0.217, batch=228\n",
      "523: loss=0.996, reward_mean=0.060, reward_bound=0.254, batch=228\n",
      "524: loss=0.998, reward_mean=0.170, reward_bound=0.286, batch=229\n",
      "525: loss=0.998, reward_mean=0.050, reward_bound=0.226, batch=230\n",
      "526: loss=0.997, reward_mean=0.090, reward_bound=0.314, batch=226\n",
      "527: loss=1.000, reward_mean=0.100, reward_bound=0.349, batch=223\n",
      "528: loss=1.012, reward_mean=0.110, reward_bound=0.387, batch=189\n",
      "529: loss=1.008, reward_mean=0.100, reward_bound=0.000, batch=199\n",
      "530: loss=1.011, reward_mean=0.110, reward_bound=0.037, batch=209\n",
      "531: loss=1.006, reward_mean=0.090, reward_bound=0.065, batch=215\n",
      "532: loss=0.995, reward_mean=0.130, reward_bound=0.170, batch=220\n",
      "533: loss=0.994, reward_mean=0.100, reward_bound=0.185, batch=222\n",
      "534: loss=0.993, reward_mean=0.070, reward_bound=0.213, batch=225\n",
      "535: loss=1.000, reward_mean=0.100, reward_bound=0.254, batch=223\n",
      "536: loss=1.003, reward_mean=0.100, reward_bound=0.282, batch=221\n",
      "537: loss=1.005, reward_mean=0.080, reward_bound=0.229, batch=224\n",
      "538: loss=1.003, reward_mean=0.110, reward_bound=0.311, batch=227\n",
      "539: loss=1.002, reward_mean=0.080, reward_bound=0.314, batch=222\n",
      "540: loss=0.999, reward_mean=0.060, reward_bound=0.185, batch=225\n",
      "541: loss=1.005, reward_mean=0.110, reward_bound=0.349, batch=217\n",
      "542: loss=1.007, reward_mean=0.080, reward_bound=0.254, batch=221\n",
      "543: loss=1.005, reward_mean=0.060, reward_bound=0.254, batch=224\n",
      "544: loss=1.004, reward_mean=0.080, reward_bound=0.349, batch=224\n",
      "545: loss=1.004, reward_mean=0.070, reward_bound=0.133, batch=227\n",
      "546: loss=1.000, reward_mean=0.050, reward_bound=0.195, batch=229\n",
      "547: loss=1.004, reward_mean=0.100, reward_bound=0.328, batch=230\n",
      "548: loss=1.002, reward_mean=0.090, reward_bound=0.376, batch=231\n",
      "549: loss=1.011, reward_mean=0.120, reward_bound=0.387, batch=219\n",
      "550: loss=1.012, reward_mean=0.120, reward_bound=0.387, batch=222\n",
      "551: loss=1.010, reward_mean=0.090, reward_bound=0.292, batch=225\n",
      "552: loss=1.010, reward_mean=0.040, reward_bound=0.153, batch=227\n",
      "553: loss=1.010, reward_mean=0.080, reward_bound=0.249, batch=229\n",
      "554: loss=1.008, reward_mean=0.080, reward_bound=0.314, batch=227\n",
      "555: loss=1.007, reward_mean=0.060, reward_bound=0.277, batch=229\n",
      "556: loss=1.011, reward_mean=0.110, reward_bound=0.387, batch=229\n",
      "557: loss=1.036, reward_mean=0.110, reward_bound=0.430, batch=152\n",
      "558: loss=1.024, reward_mean=0.150, reward_bound=0.000, batch=167\n",
      "559: loss=1.021, reward_mean=0.050, reward_bound=0.000, batch=172\n",
      "560: loss=1.015, reward_mean=0.140, reward_bound=0.000, batch=186\n",
      "561: loss=1.013, reward_mean=0.110, reward_bound=0.000, batch=197\n",
      "562: loss=1.015, reward_mean=0.050, reward_bound=0.000, batch=202\n",
      "563: loss=1.009, reward_mean=0.080, reward_bound=0.000, batch=210\n",
      "564: loss=1.006, reward_mean=0.070, reward_bound=0.022, batch=217\n",
      "565: loss=1.004, reward_mean=0.050, reward_bound=0.025, batch=222\n",
      "566: loss=1.015, reward_mean=0.060, reward_bound=0.041, batch=225\n",
      "567: loss=1.007, reward_mean=0.080, reward_bound=0.073, batch=227\n",
      "568: loss=1.009, reward_mean=0.050, reward_bound=0.126, batch=229\n",
      "569: loss=1.011, reward_mean=0.090, reward_bound=0.174, batch=230\n",
      "570: loss=1.011, reward_mean=0.160, reward_bound=0.229, batch=225\n",
      "571: loss=1.010, reward_mean=0.040, reward_bound=0.189, batch=227\n",
      "572: loss=1.010, reward_mean=0.060, reward_bound=0.206, batch=228\n",
      "573: loss=1.012, reward_mean=0.040, reward_bound=0.254, batch=220\n",
      "574: loss=1.011, reward_mean=0.070, reward_bound=0.274, batch=224\n",
      "575: loss=1.012, reward_mean=0.050, reward_bound=0.185, batch=226\n",
      "576: loss=1.012, reward_mean=0.100, reward_bound=0.282, batch=218\n",
      "577: loss=1.008, reward_mean=0.050, reward_bound=0.115, batch=222\n",
      "578: loss=1.006, reward_mean=0.020, reward_bound=0.000, batch=224\n",
      "579: loss=1.009, reward_mean=0.070, reward_bound=0.277, batch=227\n",
      "580: loss=1.008, reward_mean=0.100, reward_bound=0.282, batch=228\n",
      "581: loss=1.010, reward_mean=0.080, reward_bound=0.314, batch=218\n",
      "582: loss=1.012, reward_mean=0.070, reward_bound=0.198, batch=222\n",
      "583: loss=1.009, reward_mean=0.060, reward_bound=0.314, batch=224\n",
      "584: loss=1.008, reward_mean=0.070, reward_bound=0.254, batch=226\n",
      "585: loss=1.002, reward_mean=0.110, reward_bound=0.349, batch=213\n",
      "586: loss=1.011, reward_mean=0.070, reward_bound=0.085, batch=219\n",
      "587: loss=1.002, reward_mean=0.040, reward_bound=0.035, batch=223\n",
      "588: loss=1.000, reward_mean=0.080, reward_bound=0.117, batch=226\n",
      "589: loss=1.004, reward_mean=0.070, reward_bound=0.198, batch=228\n",
      "590: loss=1.006, reward_mean=0.080, reward_bound=0.231, batch=229\n",
      "591: loss=1.002, reward_mean=0.100, reward_bound=0.282, batch=229\n",
      "592: loss=1.001, reward_mean=0.070, reward_bound=0.314, batch=228\n",
      "593: loss=1.000, reward_mean=0.100, reward_bound=0.349, batch=228\n",
      "594: loss=1.000, reward_mean=0.110, reward_bound=0.353, batch=229\n",
      "595: loss=0.996, reward_mean=0.050, reward_bound=0.387, batch=202\n",
      "596: loss=0.995, reward_mean=0.040, reward_bound=0.000, batch=206\n",
      "597: loss=0.989, reward_mean=0.070, reward_bound=0.000, batch=213\n",
      "598: loss=0.991, reward_mean=0.060, reward_bound=0.015, batch=219\n",
      "599: loss=0.988, reward_mean=0.050, reward_bound=0.050, batch=223\n",
      "600: loss=0.994, reward_mean=0.140, reward_bound=0.301, batch=226\n",
      "601: loss=0.998, reward_mean=0.050, reward_bound=0.314, batch=225\n",
      "602: loss=0.999, reward_mean=0.060, reward_bound=0.321, batch=227\n",
      "603: loss=0.998, reward_mean=0.040, reward_bound=0.301, batch=229\n",
      "604: loss=0.998, reward_mean=0.030, reward_bound=0.192, batch=230\n",
      "605: loss=1.003, reward_mean=0.100, reward_bound=0.349, batch=225\n",
      "606: loss=1.001, reward_mean=0.080, reward_bound=0.387, batch=221\n",
      "607: loss=1.001, reward_mean=0.060, reward_bound=0.282, batch=224\n",
      "608: loss=1.002, reward_mean=0.080, reward_bound=0.380, batch=227\n",
      "609: loss=1.001, reward_mean=0.070, reward_bound=0.342, batch=229\n",
      "610: loss=1.000, reward_mean=0.060, reward_bound=0.328, batch=230\n",
      "611: loss=1.003, reward_mean=0.090, reward_bound=0.387, batch=228\n",
      "612: loss=1.001, reward_mean=0.050, reward_bound=0.297, batch=229\n",
      "613: loss=1.001, reward_mean=0.090, reward_bound=0.360, batch=230\n",
      "614: loss=1.018, reward_mean=0.040, reward_bound=0.430, batch=190\n",
      "615: loss=1.011, reward_mean=0.110, reward_bound=0.000, batch=201\n",
      "616: loss=1.007, reward_mean=0.090, reward_bound=0.000, batch=210\n",
      "617: loss=1.005, reward_mean=0.080, reward_bound=0.075, batch=217\n",
      "618: loss=1.011, reward_mean=0.060, reward_bound=0.138, batch=222\n",
      "619: loss=1.010, reward_mean=0.070, reward_bound=0.150, batch=224\n",
      "620: loss=1.009, reward_mean=0.020, reward_bound=0.000, batch=226\n",
      "621: loss=1.005, reward_mean=0.060, reward_bound=0.196, batch=228\n",
      "622: loss=1.005, reward_mean=0.040, reward_bound=0.206, batch=227\n",
      "623: loss=1.004, reward_mean=0.070, reward_bound=0.272, batch=229\n",
      "624: loss=1.003, reward_mean=0.080, reward_bound=0.282, batch=226\n",
      "625: loss=1.003, reward_mean=0.060, reward_bound=0.314, batch=223\n",
      "626: loss=1.009, reward_mean=0.080, reward_bound=0.349, batch=219\n",
      "627: loss=1.010, reward_mean=0.110, reward_bound=0.364, batch=223\n",
      "628: loss=1.015, reward_mean=0.060, reward_bound=0.134, batch=226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629: loss=1.007, reward_mean=0.070, reward_bound=0.298, batch=228\n",
      "630: loss=1.007, reward_mean=0.030, reward_bound=0.317, batch=229\n",
      "631: loss=1.004, reward_mean=0.100, reward_bound=0.387, batch=226\n",
      "632: loss=1.003, reward_mean=0.050, reward_bound=0.368, batch=228\n",
      "633: loss=1.004, reward_mean=0.090, reward_bound=0.387, batch=228\n",
      "634: loss=1.003, reward_mean=0.120, reward_bound=0.392, batch=229\n",
      "635: loss=1.010, reward_mean=0.170, reward_bound=0.430, batch=205\n",
      "636: loss=1.010, reward_mean=0.040, reward_bound=0.000, batch=209\n",
      "637: loss=1.005, reward_mean=0.100, reward_bound=0.135, batch=215\n",
      "638: loss=1.004, reward_mean=0.030, reward_bound=0.000, batch=218\n",
      "639: loss=1.005, reward_mean=0.060, reward_bound=0.137, batch=222\n",
      "640: loss=1.006, reward_mean=0.090, reward_bound=0.191, batch=225\n",
      "641: loss=1.006, reward_mean=0.040, reward_bound=0.206, batch=225\n",
      "642: loss=1.007, reward_mean=0.080, reward_bound=0.282, batch=225\n",
      "643: loss=1.007, reward_mean=0.100, reward_bound=0.314, batch=225\n",
      "644: loss=1.009, reward_mean=0.100, reward_bound=0.349, batch=225\n",
      "645: loss=1.007, reward_mean=0.080, reward_bound=0.356, batch=227\n",
      "646: loss=1.009, reward_mean=0.090, reward_bound=0.387, batch=221\n",
      "647: loss=1.010, reward_mean=0.090, reward_bound=0.387, batch=223\n",
      "648: loss=1.007, reward_mean=0.070, reward_bound=0.335, batch=226\n",
      "649: loss=1.005, reward_mean=0.090, reward_bound=0.331, batch=228\n",
      "650: loss=1.007, reward_mean=0.070, reward_bound=0.349, batch=227\n",
      "651: loss=1.007, reward_mean=0.060, reward_bound=0.349, batch=227\n",
      "652: loss=1.007, reward_mean=0.080, reward_bound=0.361, batch=229\n",
      "653: loss=1.006, reward_mean=0.070, reward_bound=0.343, batch=230\n",
      "654: loss=1.005, reward_mean=0.090, reward_bound=0.356, batch=231\n",
      "655: loss=1.006, reward_mean=0.040, reward_bound=0.430, batch=218\n",
      "656: loss=1.002, reward_mean=0.050, reward_bound=0.137, batch=222\n",
      "657: loss=1.002, reward_mean=0.050, reward_bound=0.161, batch=225\n",
      "658: loss=1.007, reward_mean=0.090, reward_bound=0.282, batch=225\n",
      "659: loss=1.006, reward_mean=0.080, reward_bound=0.246, batch=227\n",
      "660: loss=1.005, reward_mean=0.070, reward_bound=0.302, batch=229\n",
      "661: loss=1.005, reward_mean=0.080, reward_bound=0.387, batch=228\n",
      "662: loss=1.005, reward_mean=0.070, reward_bound=0.353, batch=229\n",
      "663: loss=1.005, reward_mean=0.050, reward_bound=0.387, batch=228\n",
      "664: loss=1.004, reward_mean=0.040, reward_bound=0.317, batch=229\n",
      "665: loss=1.004, reward_mean=0.050, reward_bound=0.381, batch=230\n",
      "666: loss=1.003, reward_mean=0.060, reward_bound=0.376, batch=231\n",
      "667: loss=1.003, reward_mean=0.040, reward_bound=0.387, batch=231\n",
      "668: loss=1.003, reward_mean=0.070, reward_bound=0.387, batch=231\n",
      "669: loss=1.005, reward_mean=0.120, reward_bound=0.430, batch=229\n",
      "670: loss=1.005, reward_mean=0.120, reward_bound=0.450, batch=230\n",
      "671: loss=1.022, reward_mean=0.060, reward_bound=0.478, batch=136\n",
      "672: loss=1.026, reward_mean=0.070, reward_bound=0.000, batch=143\n",
      "673: loss=1.022, reward_mean=0.060, reward_bound=0.000, batch=149\n",
      "674: loss=1.012, reward_mean=0.070, reward_bound=0.000, batch=156\n",
      "675: loss=1.005, reward_mean=0.080, reward_bound=0.000, batch=164\n",
      "676: loss=0.999, reward_mean=0.040, reward_bound=0.000, batch=168\n",
      "677: loss=0.996, reward_mean=0.060, reward_bound=0.000, batch=174\n",
      "678: loss=0.991, reward_mean=0.080, reward_bound=0.000, batch=182\n",
      "679: loss=0.987, reward_mean=0.050, reward_bound=0.000, batch=187\n",
      "680: loss=0.986, reward_mean=0.060, reward_bound=0.000, batch=193\n",
      "681: loss=0.984, reward_mean=0.080, reward_bound=0.000, batch=201\n",
      "682: loss=0.983, reward_mean=0.100, reward_bound=0.042, batch=209\n",
      "683: loss=0.980, reward_mean=0.070, reward_bound=0.023, batch=216\n",
      "684: loss=0.980, reward_mean=0.070, reward_bound=0.065, batch=221\n",
      "685: loss=0.984, reward_mean=0.090, reward_bound=0.089, batch=224\n",
      "686: loss=0.984, reward_mean=0.040, reward_bound=0.095, batch=227\n",
      "687: loss=0.985, reward_mean=0.060, reward_bound=0.122, batch=226\n",
      "688: loss=0.986, reward_mean=0.090, reward_bound=0.150, batch=226\n",
      "689: loss=0.989, reward_mean=0.060, reward_bound=0.167, batch=225\n",
      "690: loss=0.989, reward_mean=0.080, reward_bound=0.185, batch=225\n",
      "691: loss=0.991, reward_mean=0.060, reward_bound=0.206, batch=224\n",
      "692: loss=0.989, reward_mean=0.060, reward_bound=0.229, batch=223\n",
      "693: loss=0.989, reward_mean=0.060, reward_bound=0.171, batch=226\n",
      "694: loss=0.987, reward_mean=0.050, reward_bound=0.241, batch=228\n",
      "695: loss=0.986, reward_mean=0.090, reward_bound=0.254, batch=216\n",
      "696: loss=0.984, reward_mean=0.060, reward_bound=0.178, batch=221\n",
      "697: loss=0.985, reward_mean=0.090, reward_bound=0.282, batch=211\n",
      "698: loss=0.985, reward_mean=0.070, reward_bound=0.109, batch=217\n",
      "699: loss=0.983, reward_mean=0.080, reward_bound=0.224, batch=222\n",
      "700: loss=0.984, reward_mean=0.050, reward_bound=0.213, batch=225\n",
      "701: loss=0.986, reward_mean=0.040, reward_bound=0.154, batch=227\n",
      "702: loss=0.986, reward_mean=0.020, reward_bound=0.183, batch=229\n",
      "703: loss=0.986, reward_mean=0.050, reward_bound=0.229, batch=229\n",
      "704: loss=0.986, reward_mean=0.050, reward_bound=0.282, batch=229\n",
      "705: loss=0.993, reward_mean=0.090, reward_bound=0.314, batch=215\n",
      "706: loss=0.982, reward_mean=0.030, reward_bound=0.000, batch=218\n",
      "707: loss=0.996, reward_mean=0.120, reward_bound=0.286, batch=222\n",
      "708: loss=0.993, reward_mean=0.060, reward_bound=0.190, batch=225\n",
      "709: loss=0.997, reward_mean=0.130, reward_bound=0.349, batch=213\n",
      "710: loss=0.996, reward_mean=0.050, reward_bound=0.000, batch=218\n",
      "711: loss=0.997, reward_mean=0.080, reward_bound=0.229, batch=220\n",
      "712: loss=1.000, reward_mean=0.070, reward_bound=0.223, batch=224\n",
      "713: loss=1.002, reward_mean=0.100, reward_bound=0.282, batch=226\n",
      "714: loss=1.003, reward_mean=0.080, reward_bound=0.314, batch=226\n",
      "715: loss=1.003, reward_mean=0.030, reward_bound=0.316, batch=228\n",
      "716: loss=0.999, reward_mean=0.060, reward_bound=0.349, batch=225\n",
      "717: loss=0.998, reward_mean=0.070, reward_bound=0.314, batch=226\n",
      "718: loss=0.998, reward_mean=0.070, reward_bound=0.316, batch=228\n",
      "719: loss=0.999, reward_mean=0.080, reward_bound=0.349, batch=228\n",
      "720: loss=0.999, reward_mean=0.040, reward_bound=0.282, batch=228\n",
      "721: loss=1.003, reward_mean=0.070, reward_bound=0.387, batch=204\n",
      "722: loss=1.006, reward_mean=0.080, reward_bound=0.000, batch=212\n",
      "723: loss=1.005, reward_mean=0.040, reward_bound=0.000, batch=216\n",
      "724: loss=1.005, reward_mean=0.100, reward_bound=0.185, batch=219\n",
      "725: loss=1.003, reward_mean=0.050, reward_bound=0.174, batch=223\n",
      "726: loss=1.004, reward_mean=0.060, reward_bound=0.206, batch=224\n",
      "727: loss=1.005, reward_mean=0.100, reward_bound=0.282, batch=224\n",
      "728: loss=1.002, reward_mean=0.080, reward_bound=0.311, batch=227\n",
      "729: loss=1.000, reward_mean=0.030, reward_bound=0.314, batch=222\n",
      "730: loss=0.999, reward_mean=0.080, reward_bound=0.324, batch=225\n",
      "731: loss=1.001, reward_mean=0.090, reward_bound=0.349, batch=226\n",
      "732: loss=1.001, reward_mean=0.080, reward_bound=0.368, batch=228\n",
      "733: loss=1.000, reward_mean=0.080, reward_bound=0.321, batch=229\n",
      "734: loss=0.999, reward_mean=0.030, reward_bound=0.343, batch=230\n",
      "735: loss=1.001, reward_mean=0.060, reward_bound=0.387, batch=226\n",
      "736: loss=1.001, reward_mean=0.100, reward_bound=0.331, batch=228\n",
      "737: loss=1.002, reward_mean=0.050, reward_bound=0.317, batch=229\n",
      "738: loss=1.002, reward_mean=0.060, reward_bound=0.309, batch=230\n",
      "739: loss=1.001, reward_mean=0.050, reward_bound=0.349, batch=230\n",
      "740: loss=1.001, reward_mean=0.080, reward_bound=0.340, batch=231\n",
      "741: loss=1.004, reward_mean=0.070, reward_bound=0.430, batch=191\n",
      "742: loss=1.002, reward_mean=0.040, reward_bound=0.000, batch=195\n",
      "743: loss=1.001, reward_mean=0.120, reward_bound=0.175, batch=206\n",
      "744: loss=0.997, reward_mean=0.050, reward_bound=0.000, batch=211\n",
      "745: loss=0.995, reward_mean=0.060, reward_bound=0.000, batch=217\n",
      "746: loss=0.997, reward_mean=0.050, reward_bound=0.031, batch=222\n",
      "747: loss=0.992, reward_mean=0.050, reward_bound=0.062, batch=225\n",
      "748: loss=0.993, reward_mean=0.040, reward_bound=0.109, batch=229\n",
      "749: loss=0.997, reward_mean=0.060, reward_bound=0.150, batch=229\n",
      "750: loss=0.997, reward_mean=0.050, reward_bound=0.229, batch=226\n",
      "751: loss=0.997, reward_mean=0.070, reward_bound=0.282, batch=225\n",
      "752: loss=0.995, reward_mean=0.100, reward_bound=0.314, batch=223\n",
      "753: loss=0.993, reward_mean=0.090, reward_bound=0.322, batch=226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754: loss=0.992, reward_mean=0.110, reward_bound=0.331, batch=228\n",
      "755: loss=0.995, reward_mean=0.100, reward_bound=0.349, batch=218\n",
      "756: loss=0.997, reward_mean=0.050, reward_bound=0.190, batch=222\n",
      "757: loss=0.997, reward_mean=0.010, reward_bound=0.000, batch=223\n",
      "758: loss=0.995, reward_mean=0.070, reward_bound=0.220, batch=226\n",
      "759: loss=0.994, reward_mean=0.050, reward_bound=0.229, batch=227\n",
      "760: loss=0.991, reward_mean=0.030, reward_bound=0.163, batch=229\n",
      "761: loss=0.993, reward_mean=0.030, reward_bound=0.213, batch=230\n",
      "762: loss=0.998, reward_mean=0.090, reward_bound=0.314, batch=229\n",
      "763: loss=0.995, reward_mean=0.060, reward_bound=0.349, batch=228\n",
      "764: loss=0.994, reward_mean=0.040, reward_bound=0.293, batch=229\n",
      "765: loss=0.992, reward_mean=0.060, reward_bound=0.387, batch=223\n",
      "766: loss=0.990, reward_mean=0.040, reward_bound=0.183, batch=226\n",
      "767: loss=0.991, reward_mean=0.090, reward_bound=0.387, batch=227\n",
      "768: loss=0.989, reward_mean=0.050, reward_bound=0.373, batch=229\n",
      "769: loss=0.990, reward_mean=0.030, reward_bound=0.215, batch=230\n",
      "770: loss=0.989, reward_mean=0.060, reward_bound=0.376, batch=231\n",
      "771: loss=0.990, reward_mean=0.090, reward_bound=0.387, batch=230\n",
      "772: loss=0.990, reward_mean=0.070, reward_bound=0.376, batch=231\n",
      "773: loss=0.993, reward_mean=0.060, reward_bound=0.430, batch=218\n",
      "774: loss=0.995, reward_mean=0.070, reward_bound=0.260, batch=222\n",
      "775: loss=0.992, reward_mean=0.090, reward_bound=0.324, batch=225\n",
      "776: loss=0.992, reward_mean=0.070, reward_bound=0.321, batch=227\n",
      "777: loss=0.993, reward_mean=0.090, reward_bound=0.349, batch=226\n",
      "778: loss=0.997, reward_mean=0.110, reward_bound=0.409, batch=228\n",
      "779: loss=0.994, reward_mean=0.090, reward_bound=0.430, batch=224\n",
      "780: loss=0.996, reward_mean=0.030, reward_bound=0.314, batch=227\n",
      "781: loss=0.998, reward_mean=0.050, reward_bound=0.335, batch=229\n",
      "782: loss=0.997, reward_mean=0.090, reward_bound=0.364, batch=230\n",
      "783: loss=0.997, reward_mean=0.040, reward_bound=0.349, batch=230\n",
      "784: loss=0.997, reward_mean=0.050, reward_bound=0.365, batch=231\n",
      "785: loss=0.994, reward_mean=0.060, reward_bound=0.387, batch=230\n",
      "786: loss=0.995, reward_mean=0.090, reward_bound=0.430, batch=228\n",
      "787: loss=0.994, reward_mean=0.040, reward_bound=0.397, batch=229\n",
      "788: loss=0.995, reward_mean=0.080, reward_bound=0.478, batch=231\n",
      "789: loss=0.995, reward_mean=0.100, reward_bound=0.387, batch=231\n",
      "790: loss=1.006, reward_mean=0.040, reward_bound=0.478, batch=175\n",
      "791: loss=1.003, reward_mean=0.030, reward_bound=0.000, batch=178\n",
      "792: loss=1.000, reward_mean=0.070, reward_bound=0.000, batch=185\n",
      "793: loss=0.993, reward_mean=0.080, reward_bound=0.000, batch=193\n",
      "794: loss=0.993, reward_mean=0.080, reward_bound=0.000, batch=201\n",
      "795: loss=0.992, reward_mean=0.040, reward_bound=0.000, batch=205\n",
      "796: loss=0.990, reward_mean=0.090, reward_bound=0.088, batch=213\n",
      "797: loss=0.990, reward_mean=0.070, reward_bound=0.122, batch=217\n",
      "798: loss=0.988, reward_mean=0.070, reward_bound=0.150, batch=221\n",
      "799: loss=0.987, reward_mean=0.080, reward_bound=0.167, batch=224\n",
      "800: loss=0.987, reward_mean=0.090, reward_bound=0.206, batch=226\n",
      "801: loss=0.990, reward_mean=0.030, reward_bound=0.198, batch=228\n",
      "802: loss=0.993, reward_mean=0.070, reward_bound=0.229, batch=227\n",
      "803: loss=0.993, reward_mean=0.090, reward_bound=0.254, batch=228\n",
      "804: loss=0.995, reward_mean=0.070, reward_bound=0.282, batch=225\n",
      "805: loss=0.995, reward_mean=0.050, reward_bound=0.289, batch=227\n",
      "806: loss=0.994, reward_mean=0.050, reward_bound=0.224, batch=229\n",
      "807: loss=0.996, reward_mean=0.110, reward_bound=0.314, batch=222\n",
      "808: loss=0.997, reward_mean=0.070, reward_bound=0.349, batch=217\n",
      "809: loss=0.997, reward_mean=0.090, reward_bound=0.233, batch=222\n",
      "810: loss=0.999, reward_mean=0.070, reward_bound=0.324, batch=225\n",
      "811: loss=0.995, reward_mean=0.060, reward_bound=0.349, batch=225\n",
      "812: loss=1.005, reward_mean=0.050, reward_bound=0.387, batch=206\n",
      "813: loss=1.002, reward_mean=0.100, reward_bound=0.122, batch=213\n",
      "814: loss=1.001, reward_mean=0.080, reward_bound=0.220, batch=219\n",
      "815: loss=1.000, reward_mean=0.090, reward_bound=0.265, batch=223\n",
      "816: loss=0.998, reward_mean=0.020, reward_bound=0.000, batch=225\n",
      "817: loss=0.995, reward_mean=0.080, reward_bound=0.282, batch=225\n",
      "818: loss=0.996, reward_mean=0.020, reward_bound=0.063, batch=227\n",
      "819: loss=0.999, reward_mean=0.110, reward_bound=0.342, batch=229\n",
      "820: loss=0.999, reward_mean=0.010, reward_bound=0.139, batch=230\n",
      "821: loss=0.999, reward_mean=0.070, reward_bound=0.349, batch=230\n",
      "822: loss=0.998, reward_mean=0.040, reward_bound=0.340, batch=231\n",
      "823: loss=0.997, reward_mean=0.030, reward_bound=0.387, batch=226\n",
      "824: loss=1.006, reward_mean=0.130, reward_bound=0.430, batch=208\n",
      "825: loss=1.002, reward_mean=0.050, reward_bound=0.000, batch=213\n",
      "826: loss=1.003, reward_mean=0.100, reward_bound=0.220, batch=219\n",
      "827: loss=0.999, reward_mean=0.040, reward_bound=0.019, batch=223\n",
      "828: loss=0.996, reward_mean=0.050, reward_bound=0.098, batch=226\n",
      "829: loss=0.996, reward_mean=0.080, reward_bound=0.176, batch=228\n",
      "830: loss=1.002, reward_mean=0.050, reward_bound=0.254, batch=226\n",
      "831: loss=1.005, reward_mean=0.050, reward_bound=0.314, batch=224\n",
      "832: loss=1.006, reward_mean=0.050, reward_bound=0.342, batch=227\n",
      "833: loss=1.005, reward_mean=0.080, reward_bound=0.342, batch=229\n",
      "834: loss=1.004, reward_mean=0.070, reward_bound=0.349, batch=229\n",
      "835: loss=1.003, reward_mean=0.080, reward_bound=0.387, batch=224\n",
      "836: loss=1.001, reward_mean=0.100, reward_bound=0.422, batch=227\n",
      "837: loss=1.000, reward_mean=0.070, reward_bound=0.314, batch=228\n",
      "838: loss=1.001, reward_mean=0.070, reward_bound=0.349, batch=228\n",
      "839: loss=0.999, reward_mean=0.070, reward_bound=0.357, batch=229\n",
      "840: loss=0.998, reward_mean=0.080, reward_bound=0.381, batch=230\n",
      "841: loss=0.999, reward_mean=0.090, reward_bound=0.430, batch=225\n",
      "842: loss=0.999, reward_mean=0.060, reward_bound=0.349, batch=226\n",
      "843: loss=0.998, reward_mean=0.090, reward_bound=0.368, batch=228\n",
      "844: loss=0.996, reward_mean=0.050, reward_bound=0.321, batch=229\n",
      "845: loss=0.996, reward_mean=0.080, reward_bound=0.328, batch=230\n",
      "846: loss=0.996, reward_mean=0.100, reward_bound=0.376, batch=231\n",
      "847: loss=0.997, reward_mean=0.060, reward_bound=0.387, batch=231\n",
      "848: loss=0.997, reward_mean=0.090, reward_bound=0.430, batch=229\n",
      "849: loss=0.997, reward_mean=0.070, reward_bound=0.401, batch=230\n",
      "850: loss=0.994, reward_mean=0.040, reward_bound=0.247, batch=231\n",
      "851: loss=0.996, reward_mean=0.070, reward_bound=0.387, batch=231\n",
      "852: loss=0.996, reward_mean=0.050, reward_bound=0.387, batch=231\n",
      "853: loss=0.998, reward_mean=0.080, reward_bound=0.478, batch=200\n",
      "854: loss=0.996, reward_mean=0.050, reward_bound=0.000, batch=205\n",
      "855: loss=0.995, reward_mean=0.060, reward_bound=0.000, batch=211\n",
      "856: loss=0.999, reward_mean=0.050, reward_bound=0.000, batch=216\n",
      "857: loss=0.999, reward_mean=0.040, reward_bound=0.000, batch=220\n",
      "858: loss=0.988, reward_mean=0.120, reward_bound=0.146, batch=224\n",
      "859: loss=0.991, reward_mean=0.070, reward_bound=0.204, batch=227\n",
      "860: loss=0.992, reward_mean=0.050, reward_bound=0.249, batch=229\n",
      "861: loss=0.993, reward_mean=0.070, reward_bound=0.254, batch=228\n",
      "862: loss=0.991, reward_mean=0.050, reward_bound=0.282, batch=225\n",
      "863: loss=0.990, reward_mean=0.090, reward_bound=0.314, batch=224\n",
      "864: loss=0.992, reward_mean=0.110, reward_bound=0.349, batch=221\n",
      "865: loss=0.993, reward_mean=0.100, reward_bound=0.387, batch=218\n",
      "866: loss=0.992, reward_mean=0.040, reward_bound=0.021, batch=222\n",
      "867: loss=0.991, reward_mean=0.080, reward_bound=0.292, batch=225\n",
      "868: loss=0.988, reward_mean=0.100, reward_bound=0.289, batch=227\n",
      "869: loss=0.988, reward_mean=0.050, reward_bound=0.314, batch=228\n",
      "870: loss=0.988, reward_mean=0.030, reward_bound=0.241, batch=229\n",
      "871: loss=0.989, reward_mean=0.060, reward_bound=0.349, batch=228\n",
      "872: loss=0.987, reward_mean=0.110, reward_bound=0.392, batch=229\n",
      "873: loss=0.988, reward_mean=0.070, reward_bound=0.430, batch=214\n",
      "874: loss=0.982, reward_mean=0.080, reward_bound=0.164, batch=220\n",
      "875: loss=0.983, reward_mean=0.100, reward_bound=0.338, batch=224\n",
      "876: loss=0.983, reward_mean=0.090, reward_bound=0.342, batch=227\n",
      "877: loss=0.985, reward_mean=0.090, reward_bound=0.349, batch=226\n",
      "878: loss=0.983, reward_mean=0.030, reward_bound=0.153, batch=228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879: loss=0.982, reward_mean=0.020, reward_bound=0.154, batch=229\n",
      "880: loss=0.984, reward_mean=0.070, reward_bound=0.324, batch=230\n",
      "881: loss=0.983, reward_mean=0.040, reward_bound=0.338, batch=231\n",
      "882: loss=0.984, reward_mean=0.090, reward_bound=0.387, batch=230\n",
      "883: loss=0.983, reward_mean=0.090, reward_bound=0.430, batch=226\n",
      "884: loss=0.983, reward_mean=0.100, reward_bound=0.454, batch=228\n",
      "885: loss=0.983, reward_mean=0.110, reward_bound=0.397, batch=229\n",
      "886: loss=0.984, reward_mean=0.080, reward_bound=0.478, batch=232\n",
      "887: loss=0.984, reward_mean=0.030, reward_bound=0.445, batch=232\n",
      "888: loss=0.984, reward_mean=0.020, reward_bound=0.363, batch=232\n",
      "889: loss=0.990, reward_mean=0.070, reward_bound=0.478, batch=214\n",
      "890: loss=0.989, reward_mean=0.050, reward_bound=0.000, batch=219\n",
      "891: loss=0.991, reward_mean=0.070, reward_bound=0.140, batch=223\n",
      "892: loss=0.992, reward_mean=0.080, reward_bound=0.206, batch=225\n",
      "893: loss=0.991, reward_mean=0.100, reward_bound=0.321, batch=227\n",
      "894: loss=0.990, reward_mean=0.100, reward_bound=0.349, batch=227\n",
      "895: loss=0.993, reward_mean=0.050, reward_bound=0.387, batch=223\n",
      "896: loss=0.991, reward_mean=0.090, reward_bound=0.430, batch=219\n",
      "897: loss=0.993, reward_mean=0.060, reward_bound=0.292, batch=223\n",
      "898: loss=0.990, reward_mean=0.020, reward_bound=0.000, batch=225\n",
      "899: loss=0.992, reward_mean=0.110, reward_bound=0.349, batch=224\n",
      "900: loss=0.991, reward_mean=0.070, reward_bound=0.426, batch=227\n",
      "901: loss=0.993, reward_mean=0.080, reward_bound=0.387, batch=228\n",
      "902: loss=0.991, reward_mean=0.130, reward_bound=0.430, batch=225\n",
      "903: loss=0.992, reward_mean=0.110, reward_bound=0.430, batch=226\n",
      "904: loss=0.995, reward_mean=0.070, reward_bound=0.314, batch=227\n",
      "905: loss=0.995, reward_mean=0.050, reward_bound=0.349, batch=228\n",
      "906: loss=0.995, reward_mean=0.070, reward_bound=0.387, batch=227\n",
      "907: loss=0.993, reward_mean=0.070, reward_bound=0.349, batch=228\n",
      "908: loss=0.992, reward_mean=0.080, reward_bound=0.392, batch=229\n",
      "909: loss=0.992, reward_mean=0.020, reward_bound=0.229, batch=229\n",
      "910: loss=0.991, reward_mean=0.060, reward_bound=0.292, batch=230\n",
      "911: loss=0.993, reward_mean=0.130, reward_bound=0.430, batch=230\n",
      "912: loss=0.993, reward_mean=0.070, reward_bound=0.478, batch=222\n",
      "913: loss=0.994, reward_mean=0.120, reward_bound=0.400, batch=225\n",
      "914: loss=0.996, reward_mean=0.080, reward_bound=0.430, batch=224\n",
      "915: loss=0.996, reward_mean=0.070, reward_bound=0.349, batch=226\n",
      "916: loss=0.997, reward_mean=0.070, reward_bound=0.387, batch=227\n",
      "917: loss=0.995, reward_mean=0.020, reward_bound=0.071, batch=229\n",
      "918: loss=0.995, reward_mean=0.060, reward_bound=0.254, batch=229\n",
      "919: loss=0.995, reward_mean=0.070, reward_bound=0.405, batch=230\n",
      "920: loss=0.994, reward_mean=0.120, reward_bound=0.430, batch=228\n",
      "921: loss=0.993, reward_mean=0.090, reward_bound=0.478, batch=231\n",
      "922: loss=0.992, reward_mean=0.070, reward_bound=0.387, batch=231\n",
      "923: loss=0.992, reward_mean=0.080, reward_bound=0.430, batch=231\n",
      "924: loss=0.992, reward_mean=0.040, reward_bound=0.430, batch=231\n",
      "925: loss=0.992, reward_mean=0.050, reward_bound=0.430, batch=231\n",
      "926: loss=0.992, reward_mean=0.050, reward_bound=0.387, batch=231\n",
      "927: loss=0.993, reward_mean=0.060, reward_bound=0.478, batch=227\n",
      "928: loss=0.992, reward_mean=0.090, reward_bound=0.349, batch=228\n",
      "929: loss=0.992, reward_mean=0.060, reward_bound=0.484, batch=229\n",
      "930: loss=0.992, reward_mean=0.070, reward_bound=0.430, batch=229\n"
     ]
    }
   ],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "net.to(torch.device('cuda'))\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "writer = SummaryWriter(comment=\"-frozenlake\")\n",
    "\n",
    "full_batch = []\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "    if not full_batch:\n",
    "        continue\n",
    "    obs_v = torch.FloatTensor(obs).to(torch.device('cuda'))\n",
    "    acts_v = torch.LongTensor(acts).to(torch.device('cuda'))\n",
    "    full_batch = full_batch[-500:]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "        iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "    if reward_mean > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(net.to(torch.device('cuda')), (1, 16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
